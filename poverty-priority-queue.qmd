---
title: "Poverty and Priority Queueing"
author: "Marcos Gallo"
date: "2023/02/28"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    code-line-numbers: true
---

# Task Prioritization

```{r init, warning=FALSE, include=FALSE}

library(stringi)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(latticeExtra)

```

::: {.callout-important icon="false"}
## Abstract

Pre-existing literature suggests that relative material scarcity -- particularly wealth inequality -- is a significant contributing factor in dictating how people behave. We characterize irrational decision-making across impoverished groups by examining two distinct paradigms: (1) effects of inefficiency in time allocation and (2) influences of stereotyping on intersocial processes (ie. collaboration).

We experimentally model time allocation in low SES groups by administering an artificial priority queue where agents must attend to tasks with differing levels of effort, urgency and value. Observations suggest that individuals from low SES groups prioritize urgent tasks while agents from high SES groups prioritize high value tasks. Alternatively, we model intersocial collaboration by examining distribution of rewards during coalition games across agents of varying socioeconomic groups.
:::

Although income-based measures have long been the standard for assessing poverty, countless studies have since determined that purchasing power and standard of living are not accurately measured by such a metric. Rather, relative deprivation and state-trait anxiety measures are often more accurate empirical measurements of an individual's socioeconomic status (SES). In response to the evolving standards for what constitutes "poverty," we seek to establish an understanding of how relative-poverty can be self-perpetuating through poor time allocation.

Poverty traps are described as a self-perpetuating phenomena that arises when an economy is caught within a cycle that suffers from persistent undevelopment. Alternatively, we model poverty traps as an equilibrium which favors disutils over iterative cycles. To determine causal relationships between such outcomes and the behavior of low SES (type 1) agents, we examine cases of suboptimality that are distinct from high SES (type 2) agents.

To assess self-reinforcing behavior intrinsic to poverty traps, we classify and compare the choice behavior of type 1 agents in regards to their time allocation. Priority Queues, models for arranging and attending to tasks, are a tangible mechanism for capturing the choices and inter-event time agents require in regards to decision-making for daily tasks. Using the pre-existing literature surrounding poverty, we experimentally build an artificial priority queue, using a utility $u(e, u, i)$ parameterized by "effort," "urgency" and "importance."

## Priority Queuing

Time allocation is often modeled through priority queues, models to represent ordinal choices of tasks attended to based on some subjective value or utility function. Triaging bays often treat queues as a function of urgency and order of arrival, whilst donor matching programs facilitate queues based on a point-system which weights time, severity and likelihood of survival.

Two critical Priority Queuing models are developed by Cobham and Barabasi. While Cobham's model treats a queue as an infinite list of tasks, with varying task arrival times and execution rates, the Barabasi model treats the queue as a finite size list with waiting times ($\tau_w$).

$$
P(\tau_w) = \left\{
            \begin{array}{ll}
            1 - \frac{1-p^2}{4p}\ln\frac{1+p}{1-p},\\
            \frac{1-p^2}{4p(\tau_w - 1)}[(\frac{1+p}{2})^{\tau_w-1} - (\frac{1-p}{2})^{\tau_w - 1}],
            \end{array}
            \right.
$$ {#eq-prio-q}

We observe that mathematically as $p \rightarrow 0$, this distribution reduces to an inverse poisson distribution.Ultimately this expression is independent of the priority distribution - suggesting that the alginment of task priorities has no role on the time for these respective tasks to be executed.

Unfortunately, the preexisting models for priority queues are limited in regards to applications. There is negligible literature about priority queuing in marginalized groups and in regards to how priority queuing is rationalizable.

One set of literature describes priority queues as a double-sided matching market with queues that depend on priorities and also impatient customers (Castro et al 2020). In this literature, customers of high value are matched prior to those of low value, and likewise impatient customers can leave the queue if they are not matched immediately with a customer of their priority. In such queues, customers are matched according to the order they arrive in and the available participants in the other market. A notable feature of this literature is that customers of high priority are perceived as more preferred than customers who are highly impatient. Essentially after the steady-state build up of customers in the queue increases, high priority customers are queued and low-priority customers are told to wait. While this behavior is representative of many cases in real life, it does not particularly align with the behavior that the extended literature from Role Strain suggests about the decision-making choices of individuals from low SES. There is clearly a disconnect in how the average individual prioritizes tasks and likewise how poor individuals do as well.

### Urgency and Importance

The Eisenhower Matrix, a simple tool for determining optimal long-term decisions, is a simplification of how individuals determine their queue for attending to daily tasks. These matrices use dimensions of urgency (time scarcity) and importance (expected value maximization) as primary motivations for which tasks require completion. However, this model is limited. As discussed in @sec-poverty, the literature speculates the influence of effort costs in decision-making - thereby suggesting Eisenhower Matrices should be modified to account for the added parameter.

M: Explain (with equation for different weighting for warmth and competence)

#### Simulation

Simmer package:

```{r simmer-simulation}
#| fig-cap: "Probability of Survival by Urgency Weight"

urgency_survival_grid <- read_csv("urgency-survival-grid.csv") %>%
  rename(time = mean_time) %>%
  filter(!is.na(mean_survival)) %>%
  filter(time < 20) # Anything greater is close to 0

levelplot(mean_survival ~ time * weight, urgency_survival_grid, 
          panel = panel.2dsmoother(n = 200))

```

```{r simmer-burstiness}
#| fig-cap: "Burstiness by Urgency Weight"

urgency_burstiness_grid <- read_csv("urgency-burstiness-grid.csv") 

library(viridis)
ggplot(urgency_burstiness_grid,
       aes(x = M, y = B)) +
  geom_jitter(aes(color = weights),
              size = 1,
              alpha = 0.8) +
  geom_hline(yintercept = 0) +
  geom_hline(xintercept = 0) +
  labs(color = "Urgency Weight") +
  scale_color_viridis(option = "D")

```

## Relating to Poverty {#sec-poverty}

The literature suggests that the persistence of generational poverty is propagated by countless environmental, physical and psychological factors known as "Poverty Traps" (Barrett et al 2016). Though the dynamics of these negative feedback loops are heavily researched, the commonly invoked mechanism suggests that these traps exist due to a country, person or economy being unable to accumulate enough capital for incomes to rise. However, more recent literature suggests that investing additional capital within an environment will not raise the long-term steady-state income level (Kraay et al 2014) - contradicting the commonly held position. Many studies also investigate whether S-shaped curves, a great predicator of temporally dynamic relationships between income and assets, are indicative of poverty traps. However, this type of analysis has frequently yielded incorrect assessments of poverty-trapping due to a proclivity for numerical errors that such an approach poses. Given the misleading nature of many theoretical approaches to characterize poverty traps, the limited strain of literature addressing this topic has recently begun veering in the direction of seeking behavioral prescriptions.

Our analysis of poverty traps will discuss two potential causes for poverty traps: behaviors which are self-reinforcing, and those which are exacerbated by extraneous factors.

In order to synthesize the literature surrounding this topic, we will answer the following question:

-   How does the implementation of priority queuing within members of low socioeconomic groups represent distinct facets of poverty traps?

### Irrationality of Time Allocation

A particular facet of "poverty traps" can be attributed to the inefficiency of daily task management. Individuals are described as making a sequence of decisions and bargains in order to reduce the strain of their daily demands and roles in a process similar to that of an economic decision: Allocating limited resources such as energy, time, emotions, and goods (Goode, 1960) in different ways. For individuals who belonging to a higher socioeconomic class (SEC), delegating these roles becomes far easier, and the bandwidth to tackle other tasks increases significantly. The converse is true however as well: the poorer one is, the more uncontrolled demand there is on both time and money.

### Role Strain

Role strain, the "effort" dimension or subjective value individuals attribute to completing arbitrary tasks, is observed to be higher among the poor (Hopper et al 2020). Introduced in Moore's seminal paper on Role Strain, he adopts an argument that society and culture expand energy, rather than consume it. Adding tasks to already demanding circumstances will only add role strain, while adding an extra participant to assist with tasks will instead diminish the strain. A hypothesized cause of poverty traps is the proclivity for the impoverished to over-commit to high urgency tasks, thereby neglecting or under-commiting to high effort or high importance tasks. Goode concurs with this assessment, claiming that conflicts among task choices typically arise through role conflict (when two roles have demands which are mutually exclusive) or role overload (when an individual is too resource scarce to meet the demands of multiple roles). The latter holds more weightage in our research goals, an assertion we further investigate.

### Scarcity

Notably, time and resource scarcity are positively correlated with socioeconomic class. Essentially, a paucity of goods is intrinsic to individuals who are poor. The behavioral patterns which emerge as a result, align with the expected results from Role Strain literature. In countless replication studies, it has been determined that scarcity yields an increase in focus during task completion. The most prominent replication exercise conducted five experiments in which different resources were withheld in order to induce a scarcity complex (Shah et al 2012). In each game, participants were awarded a differing number of "guesses," or "attempts" as a source of currency in order to simulate either resource abundance or scarcity across players. It was observed that when players were more resource scarce, focus on the task increased as a response to the limited opportunities for success that were available. Although the efficiency and accuracy at tasks completion improved for such players, a byproduct was an increase in stress as well. This same outcome has been confirmed in alternative literature (de Bruijn et al 2021), when it was determined that for individuals who have less resources, their daily tasks become increasingly taxing and necessitate more attention (Adamkovič et al 2022).

### Cognitive Load

The cognitive fatigue that such tasks cause individuals to endure can become overwhelming. Countless literature suggests that in the aftermath of highly strenuous tasks caused by induced scarcity, attentional shifts become more commonplace. However, other studies claim that there may not be enough information to thoroughly arrive at this conclusion. Nonetheless, nearly all of the current literature converges to the understanding that poverty inadvertently results in overborrowing of resources -- particularly of those that are scarce. This phenomenon is exemplified with debt -- low SES individuals with a lower credit score are at higher risk for requesting loans.

## Relating to Anxiety

M: proposal?

## Interactions

M: proposal?

# The Experiment

-   We devised a theoretical model for how we expect rational decision-making to mainfest in our priority queuing experiement such that frequency of deviations from rationality can be monitored.

In practical applications, we can imagine an icecream parlor as a paragon example of Priority Queuing in which tasks require are imbued with varying degrees of effort, urgency and value. Effort is a tangible variable and can be measured by complexity of icecream created: additional toppings, or additional flavors. We imagine importance as the monetary value, or utility, reaped from completing a task. Urgency can be measured by customers who are impatient. We imagine such participants as having a finite time until they leave the queue and both the value and effort variable associated with their task becomes dismissed.

## Structure

Our experiment, created on Pavlovia, simulates the outlined scenario in an online format. We construct a 2-task queue which indicates what type of "order" or task an agent must attend to: (high/low) urgency, (high/low) effort, (high/low) importance. High importance is visually represented by two dollar signs, which low importance is represented by one dollar sign. Likewise, two heart represent high urgency, and one heart represents low urgency. Finally, the effort variable is characterized by the number of "clicks" and the likelihood to "squint" - real effort costs for participants - a specific task requires. This generally occurs when specific sprinkle/scoop pairings are presented. Note that the nomenclature for identifying these tasks has a three symbol standard, where the first symbol represents the value of a task (H or L for high or low), the second symbol represents the effort (H or L) and the final symbol represents the urgency (H or 0 for high or non-urgent).

![The top of the display is the 2-task queue, the bottom of the display are the clicks participants must make to successfully complete a task.](images/2ice.png){#img-choice}

For every iteration of the game that a high urgency task is unattended to, the value of the task decreases - at which point the participant leaves the queue if the value reaches no dollar signs.

To incentivize players in the game, we offer a \$1.00\$ prize which is contingent on the total number of dollar-signs, total value, players accumulate throughout the game. A counter is positioned on the display to indicate progress. Likewise, we perform 20 iterations of this experiment: sufficient time to gauge patterns within participant behavior. An important feature of this experiment is that participants can attend to tasks in any order, not necessarily left to right, which effectively should indicate how they prioritize all three of these factors. In general, our rudimentary experimental design appears as follows:

![The figure above represents the basic interface programmed for the game. Participants have a queue at the top of the game, and a series of icons they must press in order to complete a task. The hearts and dollar signs on a task represent the importance and urgency of the task and the money counter at the bottom of the screen indicates the accumulated wealth of the participant.](images/totalice.jpeg){#img-exp}

Although we do not gather inter-event data from this game <!--# can we use the RTs to calculate tau? -->, players are unaware of this feature. We expect that many participants rush and become stressed - traits of induced scarcity. To measure relative SES and relative anxiety, we use a post and intro survey to determine a relative deprivation index and also a set of STAI.

We should note that specific task situations require minimal cognition. If an individual is presented with a high value, high urgency task then they will always choose this this task. However, if an individual is presented a high importance, low urgency task and a high urgency, low importance task - it is not entirely apparent what the optimal course of action for future outcomes may be, and more so what choices individuals will make. Using R, we capture these results and output baseline charts of what the choice distribution looks like.

### Simulation

To formalize the optimal, or rational, set of behavior for maximizing rewards over long-term exposure in a queue - we developed a closed-form model of this behavior. We initially hypothesized that this decision-making process could be summarized by a simple subjective-value function, where every individual determines the priority or preference $\succeq_q$ by the subjective value utility representation. We thus developed a stochastic model, as follows:

$$
\begin{eqnarray}
    V(s) &=& \max\limits_{a}(r(s,a) + \gamma\sum\limits_{S}P(s,a, s')V(s'))\\
    S &=& \{HHU \times HHU, HHU \times HH0, LHU \times LHU, …\}\\
    r(a,s) &=& i_a - c_a\\
    R &=& {r(HHU, HHV \times HHU) = 1 - c_H, …}\\
    V(s_{final}) &=& \max\limits_{a}(r(s_{final}, a))\\
\end{eqnarray}
$$ {#eq-model}

In @eq-model the agent calculates the value of state $S'$ (which is the next state, i.e., next period). R is the reward (in dollars), $c_i$ is the cost at iteration $i$. $P$ is the probability of transitioning from state $S'$ to another state $(s'')$, given that our agents takes action $a$ in state $s'$. $V(s'')$ is the value of that new state. We subsequently sum these results over all the possible future states. Note we use $\gamma$ as our discount factor. In our normative model, we set the discounting factor to be 1.

Note that cost can include a subjective component. Instead of counting it as simply the number of clicks, we can assume that the agent may dread or fantasize about action $a$. As such, someone who likes the task might not be bothered by doing it so the cost may be very close to zero (or negative). And vice versa for someone who hates it. Similarly, we can account for substituting *subjective* probability for objective probability. That is, people might distort the actual probability of future events.

We build upon this by then generating an optimal protocol for identifying the best set of choices at any stage for maximizing long-term outcomes. Using dynamic programming, a recursive method of taking the outcomes of subgames and applying it to the entire scheme, we identify which tasks should be chosen given stage of the 2-task queue.

```{r dyn-pro-simulation-results}
#| fig-cap: "Percentage of Urgent Choices"

urgency_ratio_grid <- read_csv("urgency-ratio-grid.csv") 
urgency_ratio_grid <- urgency_ratio_grid %>%
  rename(gamma = `...1`) %>%
  pivot_longer(!gamma, names_to = "cost", values_to = "urgent") %>%
  mutate(cost = as.numeric(cost))

levelplot(urgent ~ cost * gamma, urgency_ratio_grid, 
          panel = panel.levelplot.points, cex = 0
    ) + 
    latticeExtra::layer_(panel.2dsmoother(..., n = 200))

```

## Data

We administered our experiment over both Mturk and Prolific. In our MTurk administration, we acquired N = 22 participants, and via Prolific, N = 53.

Preliminary statistics yielded the following breakdown of demographic information:

```{r}

```

K:

### Demographics

For both Mturk and Prolific, we distinguish high-low income and high-low anxiety by a median split. The high income mean is about 70k, while the low income mean is around 30k $(p<0.001)$. Likewise, the high anxiety mean score is 56.29, while low anxiety mean score is 37.17 $(p<0.001)$.

+------------------+-------+----------------------+----------------------+----------------------+
| MTurk\           | Mean  | Median               | Min                  | Max                  |
| N=22 x 20 trials |       |                      |                      |                      |
+:=================+:======+:=====================+:=====================+:=====================+
| Age              | 34.53 | 33                   | 19                   | 50                   |
+------------------+-------+----------------------+----------------------+----------------------+
| Income           |       | \$40,000 to \$49,999 | Less than \$10,000   | \$90,000 to \$99,999 |
+------------------+-------+----------------------+----------------------+----------------------+
| Education        |       | Bachelor's degree    | High school graduate | Master's degree      |
+------------------+-------+----------------------+----------------------+----------------------+
| STAI             | 44.21 | 47                   | 20                   | 67                   |
+------------------+-------+----------------------+----------------------+----------------------+

+------------------+-------+----------------------+----------------------+----------------------+
| Prolific\        | Mean  | Median               | Min                  | Max                  |
| N=53 x 20 trials |       |                      |                      |                      |
+:=================+:======+:=====================+:=====================+:=====================+
| Age              | 30.13 | 28                   | 18                   | 75                   |
+------------------+-------+----------------------+----------------------+----------------------+
| Income           |       | \$40,000 to \$49,999 | Less than \$10,000   | \$90,000 to \$99,999 |
+------------------+-------+----------------------+----------------------+----------------------+
| Education        |       | Bachelor's degree    | High school graduate | Master's degree      |
+------------------+-------+----------------------+----------------------+----------------------+
| STAI             | 45.04 | 45                   | 20                   | 68                   |
+------------------+-------+----------------------+----------------------+----------------------+

: Caption

### Measures of Poverty

M: 2022 Report

#### Self-Reported Income

#### ZIP code Measures

#### Composite Measure (PCA)

We created several questions about physical, mental, and monetary deprivation. We then summarized these results in a PCA. We use only the top two components for now, as per the Scree plot:

```{r}
#| fig-cap: pca-measure

```

##### "Dimension 1" top 5 contributors:

(Higher dimension values = higher "depression")

1.  How would you rate your mental health over the past 4 weeks? (1-5)
2.  Please select Yes or No for each of the following questions (YES = 1, NO = 2).
    a.  I am happy about achieving something good in the last 4 weeks.
    b.  Depressed and unhappy.
3.  Over the past 2 weeks, how often have you felt down, depressed, or hopeless?
4.  Over the past 2 weeks, how often have you felt little interest or pleasure in doing things?

##### "Dimension 2" top 5 contributors:

(Higher dim. values = higher "concern for mental health")

1.  How important are the following:
    a.  How you feel about yourself
    b.  Your mental state of being
    c.  Your physical state of being
2.  How important are the following aspects of your life?
    a.  I need to be mentally healthy
    b.  Having a good house to stay in

Note: We included other deprivation questions related to physical/monetary deprivation. However, those questions did not yield sufficient variance.

## Results

### Choice

#### Trade-off Decisions

When we compare an individual's choice of "challenging" tasks: decisions between high urgency - low importance or low importance - high urgency tasks, we observe that high anxiety and low income individuals more often choose high urgency tasks over high value tasks, which corroborates the current non-quanititvate literature as can be observed in the plots below.

```{r trade-off-results}
#| label: trade-off-results
#| fig-cap: "Choice proportions by income and anxiety"
#| fig-subcap:
#|   - "Mturk, Income"
#|   - "Mturk, Anxiety"
#|   - "Prolific, Income"
#|   - "Prolific, Anxiety"
#| layout-ncol: 2
#| column: page-right



```

Accuracy in trade-off decisions as a function of income and anxiety

Multiple regression models to test the same hypotheses with different measures of income.

Percent of urgent choices (vs. important) as a function of income and anxiety.

::: callout-note
## Key Result

Individuals in the low income groups and in the high anxiety groups chose Urgent tasks more often than important tasks. This result is not consistent for high income or low anxiety groups. Future graphs in this document will show that the difference-in-difference is not as clear.
:::

Here we continue looking at the interesting effects of income and anxiety on choosing the Urgent task vs the Important task. We will show:

1.  Accuracy in trade-off decisions as a function of income and anxiety

2.  Multiple regression models to test the same hypotheses with different measures of income.

3.  Percent of urgent choices (vs. important) as a function of income and anxiety.

PCA \~ Urgent Choices Analysis

It seems that when controlling for Dim 1 and 2, some effects become significant. Multicollinearity might be a problem, although correlations between STAI and PCA components are not higher than 0.6 (see correlation plots above). These results might be a result of a separation between State and Trait Anxiety. Although we only included the State questions of STAI, state and trait scores are likely correlated. PCA dimensions 1 and 2 may control for this confounding effect.

### Reaction Time

M:

### Accuracy

We also obtain a comparison of the choices participants make and the optimal, or rational, protocol we generated.

Histogram for Prolific Data. Unlike MTurk, this distribution looks normal. This score is calculated by first estimating the cost parameters for each participant and then using them with backwards induction to discover the optimal choice at each presented state.

K: explanation + histogram

```{r accuracy-histograms}
#| fig-cap: "The figure above highlights how participants typically make optimal decisions (when given 'challenging' tasks) with a probability in between 0.5 and 0.6. This is generally what we expect to observe."
```

#### Accuracy, Anxiety, and Income

Finally, we compare for each participant, how many of their choices are identical to a model that observed the full horizon for the Markov decision process. Each participant is a dot, y axis is the peerage of correct choices in the experiment while x axis is the STAI (anxiety measure). Generally the results are very similar to figure 1.5.

```{r}
#| label: accuracy-results
#| fig-cap: "Accuracy by Income and Anxiety"
#| fig-subcap:
#|   - "Mturk, Income"
#|   - "Mturk, Anxiety"
#|   - "Prolific, Income"
#|   - "Prolific, Anxiety"
#| layout-ncol: 2
#| column: page-right



```

It appears that Accuracy does not vary across income and anxiety levels with these measures. Some possibilities of this fact might include that, since time discounting here is virtually 0, leaving the important task behind will not affect participants as much (depending, of course, of the estimated cost parameters).

# Future Directions

## Power Analysis

### Behavioral Measures

IRB

### Physiological Measures

"Sartorial Symbols of Social Class Elicit Class-Consistent Behavioral and Physiological Responses: A Dyadic Approach" (Kraus et al 2014)

Goal of paper: Determine the psychological response among participants paired across socio-economic groups.

N = 128, 64 dyads (20 upper class, 20 lower class, 24 neutral condition)

DV = HRV (Heart Rate Variability) reactivity differences between high vs. low class

Prior = Upper-class = -0.69, se = 0.31; Lower-class = 0.04, se = 0.31

```{r}

# HVC across SES
Estimated sample sizes for a two-sample means test
t test assuming sd1 = sd2 = sd
HO: m2 = m1 versus Ha: m2 != m1
Study parameters:
alpha = 0.0500
power = 0.8000
delta = -0.7300
m1 = 0.0400
m2 = -0.6900
sd = 1.3860
Estimated sample sizes:
N=
N per group =
116
58
```

## Redesign

Version 2.0, we will attempt to add a scarcity component into the model:

\- Change urgency distribution

\- Change the time of incoming tasks (5.s / task)

Version 3.0, we will track for some stress/anxiety measures:

\- Skin conductance, Cortisol, Eye tracking

Eventually: Would also like to expand this and the dynamical programming to queues that are longer than length 2.

### Probabilistic Choice

As seen in @accuracy-results,

IRB

### Flexible Queue Size

IRB

### Comprehensive Measures of Poverty

IRB

As seen in @pca-measure, Revisit questionnaire to address variance problems. One possibility is to expand our sample to lower-income people. Another option is to try other questions that would elicit more variance.

## Population

### Online Sample

Pros and cons.

### Homeless

M: Look Sera's studies

IRB: Justification

### Other Low SES Cohorts

M: Proposal

# Appendix {.unnumbered}

## Fantasy and Dread Model

$$
\begin{aligned}
& V\left(s^{\prime}\right)=r(a)-c(a)+\gamma \sum_s P\left(a, s^{\prime}, s^{\prime \prime}\right) V\left(s^{\prime \prime}\right) \\
& \mathrm{SV}\left(\mathrm{s}^{\prime}\right)=\mathrm{r}(\mathrm{a})-e\left(a \mid z_e\right)+\gamma \sum_s S P(\cdot) V\left(s^{\prime}\right) \\
& e\left(a \mid z_e\right)=z_e(c(e)) \\
& \mathrm{SP}\left(a, s^{\prime}, s^{\prime \prime} \mid z_p\right)=z_p\left(P\left(a, s^{\prime}, s^{\prime \prime}\right)\right)
\end{aligned}
$$

where $z_e, z_p$ correspond to fantasy/dread and a sense of urgency, respectively. One possible

P() depends on whether unattended task "decays"

::: {#fig-prob-weighting layout-ncol="2"}
![Probability Weighting](images/prob-weighting.png){#fig-prob-weight}

![Probability Example](images/sense-urgency-illustration.png){#fig-sense-urg}

Example of Subjective Probabilities
:::

## Priority Queuing Simulation

## Dynamic Programming Simulation

## Bayesian Model for Cost Estimation

### Model

### Estimation

## Code

### Priority Queuing Simulation

```{r pri-q-simulation, eval=FALSE, include=TRUE}

library(simmer)
library(simmer.plot)
library(tidyverse)
library(latex2exp)
#library(dplyr)
#library(data.table)
set.seed(1234)

# Parameters
lambda <- 5
mu <- 5
t_simul <- 150 #Calibrated to optimize simulation time (prob_continue ≈ 0)
n_simul <- 2
scale = 100 # The scale on alpha to transfer runif(0,1) to something with more variance

weights <- seq(0, 1, by = 0.001)
arri <- list()
attr <- list()
total <- list()

# Simulations with simmer
for (w in 1:length(weights)) {
  env <- simmer("poverty")
  
  person <- trajectory("Poor's Trajectory") %>%
    set_attribute(keys = "urgency", function()
      runif(1, 0, 1)) %>%
    set_attribute(keys = "importance", function()
      runif(1, 0, 1)) %>%
    set_attribute(keys = "weight", weights[w]) %>%
    set_prioritization(function() {
      prio <-
        10000 * (
          get_attribute(env, "weight") * get_attribute(env, "urgency") +
            (1 - get_attribute(env, "weight")) * get_attribute(env, "importance")
        )
      c(prio, NA, NA)
    }) %>%
    # log_(function() {
    #   paste("Priority is: ", get_prioritization(env)[1])
    # }) %>%
    seize("person", amount = 1) %>%
    timeout(function()
      rexp(1, mu)) %>%
    release("person", amount = 1)
  
  env <-
    simmer() %>%
    add_resource("person", capacity = 1) %>%
    add_generator("Task", person, function()
      rexp(1, lambda), mon = 2)
  
  # env %>% run(until = t_simul)
  
  envs <- lapply(1:n_simul, function(i) {
    env %>%
      run(until = t_simul) %>%
      wrap()
  })
  
  ## Change Variables before
  arri[[w]] <- get_mon_arrivals(envs, ongoing = TRUE)
  attr[[w]] <- get_mon_attributes(envs)
  
  # Merge
  total[[w]] <-
    inner_join(arri[[w]], attr[[w]][attr[[w]]$key == "urgency", c(2, 4, 5)], by = c("name", "replication"))
  total[[w]] <-
    total[[w]] %>%
    rename(urgency = value)
  total[[w]] <-
    total[[w]][order(total[[w]]$replication, total[[w]]$start_time, decreasing = FALSE), ]
  row.names(total[[w]]) <- NULL
  total[[w]] <-
    inner_join(total[[w]], attr[[w]][attr[[w]]$key == "importance", c(2, 4, 5)], by = c("name", "replication"))
  total[[w]] <-
    total[[w]] %>%
    rename(importance = value)
  total[[w]] <-
    total[[w]][order(total[[w]]$replication, total[[w]]$start_time, decreasing = FALSE), ]
  row.names(total[[w]]) <- NULL
  total[[w]]$waiting_time <-
    total[[w]]$end_time - total[[w]]$start_time - total[[w]]$activity_time

    # Take care of precision problems yielding negative wait times.
  if (nrow(total[[w]][total[[w]]$waiting_time < 0 &
                      !is.na(total[[w]]$waiting_time), ]) > 0) {
    total[[w]][total[[w]]$waiting_time < 0 &
                 !is.na(total[[w]]$waiting_time), ]$waiting_time <- 0
  }

  #Weibull distribution with monomial function
  alpha = (scale - scale * as.numeric(total[[w]]$urgency)) #for now
  beta = 1
  total[[w]]$prob_fail <-
    1 - exp(-1 * (total[[w]]$waiting_time / alpha) ^ (beta))

  # Get probability of survival
  total[[w]] <- total[[w]] %>%
    mutate(prob_continue = (1 - prob_fail)) %>%
    group_by(replication) %>%
    mutate(prob_continue = lag(cumprod(prob_continue), k=1, default=1))

  cat('Simulation', w, 'of', length(weights), '\n')
}

sigma_list <- lapply(X = total, function(X)   sd(X[!is.na(X$waiting_time), ]$waiting_time))
mu_list    <- lapply(X = total, function(X) mean(X[!is.na(X$waiting_time), ]$waiting_time))
lag_time <- lapply(X = total, function(X) X %>%
                     filter(!is.na(waiting_time)) %>%
                     group_by(replication) %>%
                     mutate(waiting_time_lag = lag(waiting_time, default = 0)) %>%
                     dplyr::select(waiting_time, waiting_time_lag) %>%
                     cor())
memory   <- lapply(X = lag_time, function(X) X[2,3])


# Burstiness -------------------------------------------------------------
burstiness <- data.frame(weights) %>%
  mutate(sigma = unlist(sigma_list)) %>%
  mutate(mu = unlist(mu_list)) %>%
  mutate(B = (sigma - mu) / (sigma + mu)) %>%
  mutate(M = unlist(memory))

# Probability of Survival -------------------------------------------------
prob_survival <- lapply(X = total,
                        function(X) X %>%
                          group_by(name) %>%
                          summarise(mean_survival = mean(prob_continue),
                                    mean_time = mean(end_time)) %>%
                          rename(task = name) %>%
                          mutate(task = readr::parse_number(task))
                          )
names(prob_survival) <- weights
survival <- prob_survival %>%
  bind_rows(.id = "weight")

```

### Dynamic Programming Simulation

```{r dyn-pro-simulation, eval=FALSE, include=TRUE}

library(stringi)
library(stringr)

#Importance
imp <- c(1,2)
#Cost
cost1 = 0
cost2_grid <- seq(from = 0.1, to = 0.9, by = 0.05)
#Discounting
gamma_grid <- seq(from = 0.8, to = 1, by = 0.025)
#Choices
tasks <- c("HHU",
            "HH0",
            "HLU",
            "HL0",
            "LHU",
            "LH0",
            "LLU",
            "LL0")
tasks <- as.data.frame(tasks)
# Urgency Ratios
ratios <- c()

for (gamma in gamma_grid) {
  print(gamma)
  for (cost2 in cost2_grid) {
    # Add values
    tasks$value  <- c(imp[2]-cost2,
                      imp[2]-cost2,
                      imp[2]-cost1,
                      imp[2]-cost1,
                      imp[1]-cost2,
                      imp[1]-cost2,
                      imp[1]-cost1,
                      imp[1]-cost1)
    #Look up values
    r_choice <- tasks$value
    names(r_choice) <- tasks$tasks
    ## Use unname(r_choice["HH0"]) for example
    
    # States
    S <- as.data.frame(t(combn(tasks$tasks,2)))
    for (chr in tasks$tasks) {
      # Add states with two of the same task
      S <- rbind(S,c(chr,chr))
      # Add states with only one task transition to one task
      S <- rbind(S,c(chr,NA))
      # Add states with only one task transitioning to two tasks
      S <- rbind(S,c(chr,"TR"))
    }
    
    #Choice|States
    ## 1st column is choice.
    ## 1st+2nd columns are the state
    ch <- expand.grid(tasks$tasks,tasks$tasks)
    # Fix factor levels for next step
    levels(ch$Var2)
    levels(ch$Var2) = c("HHU", "HH0", "HLU", "HL0", "LHU", "LH0", "LLU", "LL0", "TR")
    for (chr in tasks$tasks) {
      ch <- rbind(ch,c(chr,NA))
      ch <- rbind(ch,c(chr,"TR"))
    }
    
    # Transition Probabilities
    choices <- paste(ch$Var1,ch$Var2,sep = "x")
    states <- paste(S$V1,S$V2,sep = "x")
    transitions <- expand.grid(choices,states)
    # Remaining task (last 3 characters) is in the next state, but no NA
    transitions$prob <- NA
    ## Non-urgent tasks:
    ## Note this is also transitioning to NA and TR states. Fix it in line 91 and __
    transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$prob <- 
      str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var2,
                 pattern = str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var1,-3))
    ## Urgent tasks: 
    ### If HXU -> LXU
    transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                  str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$prob <-
      str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                     str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var2,
                 pattern = paste("L",
                                 str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                                       str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var1,
                                         start = -2,
                                         end = -2),"U",sep = ""))
    ### If LXU -> NA
    ## first change all transitions to false
    transitions[str_detect(str = str_sub(transitions$Var2,-3), pattern = "NA"),]$prob <- FALSE
    # Now calculate other transitions
    transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                  str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$prob <-
      str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                     str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$Var2,
                 pattern = "NA")
    
    ## Single task with transition to single task:
    transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$prob <-
      str_detect(str = transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$Var2,
                 pattern = "TR")
    ## Single task with transition to single task:
    transitions[str_detect(str = transitions$Var1, pattern = "TR"),]$prob <- 0
    
    #Transforming them into probabilities by dividing by 8
    transitions$prob <- as.numeric(transitions$prob)
    transitions$prob <- transitions$prob / 8
    
    ## Single task with transition to single task:
    transitions[str_detect(str = transitions$Var1, pattern = "TR") &
                  str_length(transitions$Var2) == 7,]$prob <- 1/36
    
    ##################################################
    ################# Simulation
    ##################################################
    
    # Backwards induction
    ## Determine number of loops
    rounds = 20
    ## Create column with values of each choice for each round
    col_names <- paste("rd", c(1:rounds), sep = "")
    ch[col_names] <- NA
    ## Last round:
    ### fill in column in "ch" with the values
    for(choice in tasks$tasks) {
      ### first column is choice: find in tasks table the value of that choice
      ch[ch$Var1 == choice,rounds+2] <- tasks[tasks$tasks == choice,]$value
    }
    ### Multiply by discount factor^round
    ch[,rounds+2] <- ch[,rounds+2]*gamma^(rounds-1)
    
    
    ## Create state values for each round
    S[col_names] <- NA
    ### for each state in "S" choose the highest value in "ch" and create another column with the values
    best_option <- function(round, choice_matrix = ch, state_matrix = S, label_matrix = label){
      for(row in c(1:nrow(state_matrix))) {
        if (is.na(state_matrix[row,]$V2)) {
          state_matrix[row,round + 2] <- choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                         is.na(choice_matrix$Var2),round +2]
          label_matrix[row,round + 2] <- as.character(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 & 
                                                                      is.na(choice_matrix$Var2), 1])
        } else if (as.character(state_matrix[row,]$V2) == "TR") {
          state_matrix[row,round + 2] <- na.exclude(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                                    choice_matrix$Var2 == state_matrix[row,]$V2,round + 2])
          label_matrix[row,round + 2] <- as.character(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                                      is.na(choice_matrix$Var2), 1])
        } else {
          choice1 <- choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                     choice_matrix$Var2 == state_matrix[row,]$V2 &
                                     !is.na(choice_matrix$Var2) &
                                     as.character(choice_matrix$Var2) != "TR",round +2]
          choice2 <- choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V2 &
                                     choice_matrix$Var2 == state_matrix[row,]$V1 &
                                     !is.na(choice_matrix$Var2) &
                                     as.character(choice_matrix$Var2) != "TR",round +2]
          if (choice1 > choice2) {
            state_matrix[row,round + 2] <- choice1
            # ### save choices: change the suboptimal choices to NA
            # choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V2 &
            #                 choice_matrix$Var2 == state_matrix[row,]$V1 &
            #                 !is.na(choice_matrix$Var2) &
            #                 as.character(choice_matrix$Var2) != "TR",round +2] <- NA
            label_matrix[row,round + 2] <- as.character(na.exclude(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                                        choice_matrix$Var2 == state_matrix[row,]$V2, 1]))
          } else if (choice1 < choice2) {
            state_matrix[row,round + 2] <- choice2
            # ### save choices: change the suboptimal choices to NA
            # choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
            #                 choice_matrix$Var2 == state_matrix[row,]$V2 &
            #                 !is.na(choice_matrix$Var2) &
            #                 as.character(choice_matrix$Var2) != "TR",round +2] <- NA
            label_matrix[row,round + 2] <- as.character(na.exclude(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V2 &
                                                                        choice_matrix$Var2 == state_matrix[row,]$V1, 1]))
          } else if (choice2 == choice1){
            state_matrix[row,round + 2] <- choice1
            label_matrix[row,round + 2] <- "either"
          }
        }
      }
      return(list(state_matrix,choice_matrix,label_matrix))
    }
    
    label <- S
    new_states_choices <- best_option(rounds)
    S <- new_states_choices[[1]]
    ch <- new_states_choices[[2]]
    label <- new_states_choices[[3]]
    ## second to last round to first round
    ### loop through the choices|states in "S"
    ### For each choice|state, calculate the the expected value
    
    # We need to change "transitions" for simplicity of search
    transitions$choice <- substr(transitions$Var1, 1, 3)
    transitions$non_choice <- substr(transitions$Var1, 5, 7)
    transitions[transitions$non_choice == "NA",]$non_choice <- NA
    transitions$state2_1 <- substr(transitions$Var2, 1, 3)
    transitions$state2_2 <- substr(transitions$Var2, 5, 7)
    transitions[transitions$state2_2 == "NA",]$state2_2 <- NA
    
    for(i in c(1:(rounds-1))){
      values <- S[,c(1,2,(rounds+3-i))]
      for(row in c(1:nrow(ch))){
        #### create probability vector P [44x1] from "transitions"
        if(!is.na(ch[row,2])) {
          prob_weight <- 
            transitions[ch[row,1] == transitions$choice &
                          ch[row,2] == transitions$non_choice &
                          !is.na(ch[row,2]) &
                          !is.na(transitions$non_choice),]
        } else if(is.na(ch[row,2])) {
          prob_weight <- 
            transitions[ch[row,1] == transitions$choice &
                          is.na(ch[row,2]) &
                          is.na(transitions$non_choice),]
        }
        
        #### Make sure "S" is in the same order [1x44]
        prob_weight <- merge(x = prob_weight,
                             y = values,
                             by.x = c("state2_1", "state2_2"),
                             by.y = c("V1", "V2"))
        prob_weight$weighted_value <- prob_weight$prob * values[,c(length(values))]
        ### Multiply by discount factor^round
        ch[row,rounds+2-i] <- gamma^(rounds-1-i)*tasks[tasks$tasks == ch[row,1],]$value + sum(prob_weight$weighted_value)
      }
      new_states_choices <- best_option(rounds-i)
      S <- new_states_choices[[1]]
      ch <- new_states_choices[[2]]
      label <- new_states_choices[[3]]
    }
    urgency <- apply(label[,-c(1:2)], 2, function(X) substr(X, nchar(X), nchar(X)))
    ratios <- append(ratios, table(urgency)["U"]/sum(table(urgency)))
  }
}

ratios_grid <- matrix(ratios,
                      nrow = length(gamma_grid),
                      ncol = length(cost2_grid),
                      dimnames = list(gamma_grid,
                                      cost2_grid))

### Export labels
write.csv(ratios_grid,"./urgency-ratio-grid.csv")

```

### Bayesian Model for Cost Estimation

#### Model

```{stan cost-model, eval=FALSE, include=TRUE, output.var="my_model"}
data {
  int<lower=1> N;
  int<lower=1> T;
  int<lower=2> nOpt;
  int<lower=1, upper=T> Tsubj[N];
  int<lower=0, upper=nOpt> choice[N, T]; //left or right?
  int<lower=0, upper=80> opt_st[N, T]; //option-state: an easy way to map the choices for choice prob calculation
  int value_lookup[80];
  int state_lookup[52, nOpt];
  matrix<lower=0, upper = 1>[80, 52] prob_weight;
  int<lower=0, upper=80> counterpart[80];
  // real outcome[N, T];  // no lower and upper bounds
}
transformed data {
  vector[nOpt] initV;  // initial values for EV
  initV = rep_vector(0.0, nOpt);
}
parameters {
// Declare all parameters as vectors for vectorizing
  // Hyper(group)-parameters
  vector[2] mu_pr;
  vector<lower=0>[2] sigma;

  // Subject-level parameters (for transformation from hyper to subj parameter)
  vector[N] costL;  // cost_low
  vector[N] costH;    // cost_high
}
model {
  // Hyperparameters
  mu_pr  ~ normal(0, 5); //weakly informative priors
  sigma ~ gamma(2,0.1); //weakly informative priors

  // individual parameters
  for (i in 1:N) {
    //tau[i]   = Phi_approx(mu_pr[1]  + sigma[1]  * tau_pr[i]); //approx Normal CDF + noise
    costL[i] ~ normal(mu_pr[1], sigma[1]);
    costH[i] ~ normal(mu_pr[2], sigma[2]);
  }

  // subject loop and trial loop
  for (i in 1:N) {
    vector[nOpt] ev; // expected value
    vector[4] value; // vector of value option, lookup table
    matrix[80, Tsubj[i]] ch; 
    matrix[52, Tsubj[i]] st;
    int round_back; // backwards counter for induction
    real weighted_value;
    
    ev = initV;

    // Declaring values for each option, lookup table (make it loop later)
    value[1] = 2 - costH[i];
    value[2] = 1 - costH[i];
    value[3] = 1 - costL[i];
    value[4] = 2 - costL[i];


    // Backwards induction
    //  fill in column in "ch" with the values
    for(option in 1:80) {
      // first column is choice: find in tasks table the value of that choice
      // lookup is a vector that tells you which cost correspondends to each choice
      ch[option, Tsubj[i]] = value[value_lookup[option]];
    }
    //    Create state values for each round
    // state_lookup tells you which choice|state maps onto which state
    // for each state in "S" choose the highest value in "ch" and create another column with the values
    for(state in 1:52) {
      if (ch[state_lookup[state,1], Tsubj[i]] >= ch[state_lookup[state,2], Tsubj[i]]) {
        st[state, Tsubj[i]] = ch[state_lookup[state,1], Tsubj[i]];
      } else if (ch[state_lookup[state,1], Tsubj[i]] < ch[state_lookup[state,2], Tsubj[i]]) {
        st[state, Tsubj[i]] = ch[state_lookup[state,2], Tsubj[i]];
      }
    }
        // compute action probabilities
        ev[1] = ch[opt_st[i, Tsubj[i]], Tsubj[i]];
        ev[2] = ch[counterpart[opt_st[i, Tsubj[i]]], Tsubj[i]];
        choice[i, Tsubj[i]] ~ categorical_logit(ev);
        
        for (t in 1:(Tsubj[i]-1)) {
          round_back = Tsubj[i] - t;
          for(option in 1:80) {
            // use action probabilities
            weighted_value = dot_product(prob_weight[option], col(st, (round_back + 1)) );
            ch[option, round_back] = value[value_lookup[option]] + weighted_value;
          }
          for(state in 1:52) {
            if (ch[state_lookup[state,1], round_back] >= ch[state_lookup[state,2], round_back]) {
              st[state, round_back] = ch[state_lookup[state,1], round_back];
            } else if (ch[state_lookup[state,1], round_back] < ch[state_lookup[state,2], round_back]) {
              st[state, round_back] = ch[state_lookup[state,2], round_back];
            }
          }
          // compute action probabilities
          ev[1] = ch[opt_st[i, round_back], round_back];
          ev[2] = ch[counterpart[opt_st[i, round_back]], round_back];
          choice[i, round_back] ~ categorical_logit(ev);
        }
  }
  
}
```

#### Model Data

```{r cost-model-data, eval=FALSE, include=TRUE}
# Data Wrangling ----------------------------------------------------------
load("Prolific-pilot1.Rdata")
data <- pilot1[!is.na(pilot1$participant),]
data <- data %>%
  group_by(participant) %>%
  mutate(COUNTER = row_number())
Tsubj <- data %>%
  group_by(participant) %>%
  summarize(n = n())

# Creating Choice set ----------------------------------------------------------
choices <- c("HHU", "HH0", "HLU", "HL0", "LHU", "LH0", "LLU", "LL0")
ch <- expand.grid(choices, choices)
# Fix factor levels for next step
levels(ch$Var2) = append(choices, "TR")
for (chr in choices) {
  ch <- rbind(ch,c(chr,NA))
  ch <- rbind(ch,c(chr,"TR"))
}

# Creating State set ----------------------------------------------------------
S <- as.data.frame(t(combn(choices,2)))
for (chr in choices) {
  # Add states with two of the same task
  S <- rbind(S,c(chr,chr))
  # Add states with only one task transition to one task
  S <- rbind(S,c(chr,NA))
  # Add states with only one task transitioning to two tasks
  S <- rbind(S,c(chr,"TR"))
}

# Create Transition Set ----------------------------------------------------------
# Transition Probabilities
choices <- paste(ch$Var1,ch$Var2,sep = "x")
states <- paste(S$V1,S$V2,sep = "x")
transitions <- expand.grid(choices,states)
# Remaining task (last 3 characters) is in the next state, but no NA
transitions$prob <- NA
## Non-urgent tasks:
## Note this is also transitioning to NA and TR states. Fix it in line 91 and __
transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$prob <- 
  str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var2,
             pattern = str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var1,-3))
## Urgent tasks: 
### If HXU -> LXU
transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
              str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$prob <-
  str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                 str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var2,
             pattern = paste("L",
                             str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                                   str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var1,
                                     start = -2,
                                     end = -2),"U",sep = ""))
### If LXU -> NA
## first change all transitions to false
transitions[str_detect(str = str_sub(transitions$Var2,-3), pattern = "NA"),]$prob <- FALSE
# Now calculate other transitions
transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
              str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$prob <-
  str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                 str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$Var2,
             pattern = "NA")

## Single task with transition to single task:
transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$prob <-
  str_detect(str = transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$Var2,
             pattern = "TR")
## Single task with transition to single task:
transitions[str_detect(str = transitions$Var1, pattern = "TR"),]$prob <- 0

#Transforming them into probabilities by dividing by 8
transitions$prob <- as.numeric(transitions$prob)
transitions$prob <- transitions$prob / 8

## Single task with transition to single task:
transitions[str_detect(str = transitions$Var1, pattern = "TR") &
              str_length(transitions$Var2) == 7,]$prob <- 1/36
# We need to change "transitions" for simplicity of search
transitions$choice <- substr(transitions$Var1, 1, 3)
transitions$non_choice <- substr(transitions$Var1, 5, 7)
transitions[transitions$non_choice == "NA",]$non_choice <- NA
transitions$state2_1 <- substr(transitions$Var2, 1, 3)
transitions$state2_2 <- substr(transitions$Var2, 5, 7)
transitions[transitions$state2_2 == "NA",]$state2_2 <- NA


# create choice[N,T] ----------------------------------------------------------
data$choice_bin <- 1
data[data$choice == "R",]$choice_bin <- 2
choice <- data %>%
  select(participant, COUNTER, choice_bin) %>%
  group_by(COUNTER) %>%
  spread(COUNTER, choice_bin)
# Take NAs out
choice[is.na(choice)] <- 0


# Option-State ------------------------------------------------------------
## an easy way to map the choices for choice prob calculation
ch_opt_translation <- ch[,c(1:2)]
ch_opt_translation$index <- c(1:length(ch_opt_translation$Var1))
ch_opt_translation$i1 <- NA
ch_opt_translation$e1 <- NA
ch_opt_translation$i2 <- NA
ch_opt_translation$e2 <- NA

ch_opt_translation$u1 <- grepl("U", ch_opt_translation$Var1)
ch_opt_translation$u2 <- grepl("U", ch_opt_translation$Var2)
ch_opt_translation[is.na(ch_opt_translation$Var2), ]$u2 <- NA
ch_opt_translation$u1 <- as.numeric(ch_opt_translation$u1) + 1
ch_opt_translation$u2 <- as.numeric(ch_opt_translation$u2) + 1

ch_opt_translation$i1 <- ifelse(substring(ch_opt_translation$Var1, 1, 1) == "H", 2, 1)
ch_opt_translation$i2 <- ifelse(substring(ch_opt_translation$Var2, 1, 1) == "H", 2, 1)
ch_opt_translation[is.na(ch_opt_translation$Var2), ]$i2 <- NA

ch_opt_translation$e1 <- ifelse(substring(ch_opt_translation$Var1, 2, 2) == "H", 2, 1)
ch_opt_translation$e2 <- ifelse(substring(ch_opt_translation$Var2, 2, 2) == "H", 2, 1)
ch_opt_translation[is.na(ch_opt_translation$Var2), ]$e2 <- NA

ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$i2 <- 
  ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$i1
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$e2 <- 
  ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$e1
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$u2 <- 
  ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$u1

ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$i1 <- NA
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$e1 <- NA
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$u1 <- NA

data <- merge(x = data,
              y = ch_opt_translation[,-c(1:2)],
              by = c("u1", "u2", "e1", "e2", "i1", "i2"))

opt_st <- data %>%
  select(participant, COUNTER, index) %>%
  group_by(COUNTER) %>%
  spread(COUNTER, index)
# Take NAs out
opt_st[is.na(opt_st)] <- 0

ch_opt_translation$value <- 0
ch_opt_translation[grepl("HH", ch_opt_translation$Var1),]$value = 1
ch_opt_translation[grepl("LH", ch_opt_translation$Var1),]$value = 2
ch_opt_translation[grepl("LL", ch_opt_translation$Var1),]$value = 3
ch_opt_translation[grepl("HL", ch_opt_translation$Var1),]$value = 4


S$st_index <- c(1:nrow(S))
S <- merge(x = S,
           y = ch_opt_translation[,c(1:3)],
           by.x = c("V1", "V2"),
           by.y = c("Var1", "Var2"))
ch_opt_translation$index2 <- ch_opt_translation$index
S <- merge(x = S, 
           y = ch_opt_translation[,c(1:2,11)],
           by.x = c("V2", "V1"),
           by.y = c("Var1", "Var2"),
           all.x = TRUE)
state_lookup <- S %>%
  select(st_index, index, index2)
# Take NAs out
state_lookup[is.na(state_lookup$index2),]$index2 <- state_lookup[is.na(state_lookup$index2),]$index
# Order state_lookup
state_lookup <- state_lookup %>%
  arrange(order_by = st_index)

prob_weight <- merge(x = transitions,
                      y = ch_opt_translation[,c(1:3)],
                      by.x = c("choice", "non_choice"),
                      by.y = c("Var1", "Var2"))
prob_weight <- merge(x = prob_weight,
                     y = ch_opt_translation[,c(1:2,11)],
                     by.x = c("state2_1", "state2_2"),
                     by.y = c("Var1", "Var2"))


# Map Choices to States ---------------------------------------------------
choice_state <- as.data.frame(c(1:80))
choice_state <- merge(x = choice_state,
                      y = state_lookup[,-c(3)],
                      by.x = c("c(1:80)"),
                      by.y = c("index"),
                      all.x = TRUE)
choice_state <- merge(x = choice_state,
                      y = state_lookup[,-c(2)],
                      by.x = c("c(1:80)"),
                      by.y = c("index2"),
                      all.x = TRUE)
choice_state$st_index <- choice_state$st_index.x
choice_state[is.na(choice_state$st_index.x),]$st_index <- choice_state[is.na(choice_state$st_index.x),]$st_index.y
choice_state <- choice_state %>%
  select(`c(1:80)`, st_index)
prob_weight <- merge(x=prob_weight,
                     y=choice_state,
                     by.x = "index2",
                     by.y= "c(1:80)")
prob_weight <- prob_weight %>%
  select(index, st_index, prob)

prob_weight <- prob_weight %>%
  group_by(index) %>%
  spread(st_index, prob)


# Counterpart choice|states -----------------------------------------------
counterpart <- merge(x = choice_state, y = state_lookup[,-c(1)], by.x = "c(1:80)", by.y = "index", all.x = TRUE)
counterpart <- merge(x = counterpart, y = state_lookup[,-c(1)], by.x = "c(1:80)", by.y = "index2", all.x = TRUE)
counterpart$index_ALL <- counterpart$index
counterpart[is.na(counterpart$index),]$index_ALL <- counterpart[is.na(counterpart$index),]$index2
counterpart <- counterpart %>%
  select(index_ALL)

model_data <- list( N = length(unique(data$participant)), #number of part
                    T = max(data$COUNTER), # number of max rounds
                    nOpt = 2,
                    Tsubj = Tsubj$n, # number of round
                    choice = choice[,c(-1)],
                    opt_st = opt_st[,c(-1)],
                    value_lookup = ch_opt_translation$value,
                    state_lookup = state_lookup[,c(-1)],
                    prob_weight = prob_weight[,c(-1)],
                    counterpart = counterpart$index_ALL) # transition probabilities

```

#### Estimation

```{r cost-model-estimation, eval=FALSE, include=TRUE}

load("prolific1StanData.Rdata")
# my_model <- stan_model(file = "recipes/hier-bayes-simple.stan", verbose = TRUE)
Prolific1_Stan_results <- sampling(object = my_model, data = model_data,
                                   iter = 1000,
                                   chains = 1,
                                   cores = 3)

```
