---
title: "Poverty and Priority Queueing"
author: "Marcos Gallo"
date: "2023/02/28"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    code-line-numbers: true
---

# Task Prioritization

```{r init, warning=FALSE, include=FALSE}

library(stringi)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(latticeExtra)
library(rstan)

load("./joint_pilot.Rdata")
load("./prolific1StanData.Rdata")
load("./MTurk_sample_nolimit2.Rdata")
```

::: {.callout-important icon="false"}
## Â¶Abstract

Pre-existing literature suggests that relative material scarcity -- particularly wealth inequality -- is a significant contributing factor in dictating how people behave. We characterize irrational decision-making across impoverished groups by examining two distinct paradigms: (1) effects of inefficiency in time allocation and (2) influences of stereotyping on intersocial processes (ie. collaboration).

We experimentally model time allocation in low SES groups by administering an artificial priority queue where agents must attend to tasks with differing levels of effort, urgency and value. Observations suggest that individuals from low SES groups prioritize urgent tasks while agents from high SES groups prioritize high value tasks. Alternatively, we model intersocial collaboration by examining distribution of rewards during coalition games across agents of varying socioeconomic groups.
:::

Poverty is a multi-dimensional problem, and behavioral scientists have focused on the effects of the environment on the decision-making process across socio-economic levels (Bertrand et al., 2004). Their underlying assumption is that humans share the same underlying biology and psychology, and that "poverty" causes significant changes in people's environments and, in turn, their decisions. For example, living in poverty often means having a limited set of options in life (from choosing what to have for dinner to which occupation to pursue), and it increases the number of demands in terms of time and mental resources (Banerjee & Duflo, 2012). This lack of control over choices leads to a chronic state of vigilance and stress (Chemin et al., 2013).

In this paper, we focus on the trade-offs between urgency and importance in day-to-day decisions. It is hypothesized that, in the mass of to-dos and responsibilities, those with limited resources have to leave specific tasks undone. Consequently, we argue that they may not have time to complete all tasks or the necessary money and, as a result, must decide what to prioritize in their lives. Although one could theoretically calculate the most optimal course of action, it is improbable that individuals would take this actuarial approach. Instead, we argue that they are more likely to use heuristics or intuition to decide what problem to address, where to spend money, and what to leave undone. Such heuristics, however, are prone to fail under particular circumstances.

We argue that, under the strain faced by the poor, a series of behaviors and physiological phenomena arise. Making "urgent versus important" trade-offs causes stress and anxiety levels to increase and shift attention towards specific features of a choice. We propose that an environment of poverty shifts the weight placed on the urgency of a task (e.g., estimating that urgent tasks will "come back to bite" much faster than others and estimating that nonurgent opportunities are unlikely to disappear or change). This urgency mindset may increase stress and anxiety levels, leading the poor to spend significant portions of their income on tobacco, alcohol, and lotteries (Banerjee & Duflo, 2007; Blalock et al., 2007; Haisley et al., 2008; World Health Organization, 2021). Individuals in low socio-economic groups tend to save too little (Shurtleff, 2009) frequently borrow at exorbitant interest rates (Banerjee & Duflo, 2007; Skiba & Tobacman, 2008), and limit preventive healthcare services (Lusardi et al., 2010).

## Poverty Traps

We argue that the poor are caught in an "urgency trap," where neglect of nonurgent tasks may cause an increase of urgency in the future. For example, not undergoing regular medical check-ups may increase susceptibility to unexpected illnesses.

This phenomenon is closely related to the theory of poverty traps. This framework defines a self-perpetuating phenomena that arise when an economy is caught within a cycle that suffers from persistent undevelopment. In this paper, we model poverty traps as an equilibrium which favors disutility over iterative cycles. To determine causal relationships between such outcomes and the behavior of low SES (type 1) agents, we examine cases of suboptimality that are distinct from high SES (type 2) agents. To assess self-reinforcing behavior intrinsic to poverty traps, we classify and compare the choice behavior of type 1 agents in regards to their time allocation.

## Priority Queuing

Time allocation is often modeled through priority queues, models to represent ordinal choices of tasks attended to based on some subjective value or utility function. Triaging bays often treat queues as a function of urgency and order of arrival, whilst donor matching programs facilitate queues based on a point-system which weights time, severity and likelihood of survival.

Two critical Priority Queuing models are developed by Cobham and Barabasi. While Cobham's model treats a queue as an infinite list of tasks, with varying task arrival times and execution rates, the Barabasi model treats the queue as a finite size list with waiting times ($\tau_w$).

$$
P(\tau_w) = \left\{
            \begin{array}{ll}
            1 - \frac{1-p^2}{4p}\ln\frac{1+p}{1-p},\\
            \frac{1-p^2}{4p(\tau_w - 1)}[(\frac{1+p}{2})^{\tau_w-1} - (\frac{1-p}{2})^{\tau_w - 1}],
            \end{array}
            \right.
$$ {#eq-prio-q}

We observe that mathematically as $p \rightarrow 0$, this distribution reduces to an inverse poisson distribution.Ultimately this expression is independent of the priority distribution - suggesting that the alginment of task priorities has no role on the time for these respective tasks to be executed.

Unfortunately, the preexisting models for priority queues are limited in regards to applications. There is negligible literature about priority queuing in marginalized groups and in regards to how priority queuing is rationalizable.

One set of literature describes priority queues as a double-sided matching market with queues that depend on priorities and also impatient customers (Castro et al 2020). In this literature, customers of high value are matched prior to those of low value, and likewise impatient customers can leave the queue if they are not matched immediately with a customer of their priority. In such queues, customers are matched according to the order they arrive in and the available participants in the other market. A notable feature of this literature is that customers of high priority are perceived as more preferred than customers who are highly impatient. Essentially after the steady-state build up of customers in the queue increases, high priority customers are queued and low-priority customers are told to wait. While this behavior is representative of many cases in real life, it does not particularly align with the behavior that the extended literature from Role Strain suggests about the decision-making choices of individuals from low SES. There is clearly a disconnect in how the average individual prioritizes tasks and likewise how poor individuals do as well.

### Urgency and Importance

Although some scientists have proposed a "Scarcity" view of poverty (Mullainathan & Shafir, 2014), their conclusions have not withstood the scrutiny of science, leaving the causes and mechanisms of these phenomena largely unexplained (O'Donnell et al., 2021). We seek a more rigorous understanding of how relative-poverty can be self-perpetuating through poor time allocation. We explain prioritization of day-to-day tasks as an attempt to balance two requirements: (1) maximize life satisfaction given one's limited resources, and (2) minimize the potentially harmful effects of leaving tasks undone change with time. We call the former requirement "importance" and the latter "urgency" (Bratterud et al., 2020).

The Eisenhower Matrix (see @app-eisenhower for details), a simple tool for determining optimal long-term decisions, is a simplification of how individuals determine their queue for attending to daily tasks. This framework uses dimensions of urgency (time scarcity) and importance (expected value maximization) as primary motivations for which tasks require completion. We intend to characterize the mapping between urgency $U$ and importance $I$ , namely $\{ U,I \} \mapsto \mathbb{R}$. As discussed in @sec-poverty, the literature speculates the influence of effort costs in decision-making - thereby suggesting Eisenhower Matrices should be modified to account for the added parameter.

#### Simulation

Priority Queues models are a tangible mechanism for capturing the choices and inter-event time agents require in regards to decision-making for daily tasks. Using the pre-existing literature surrounding poverty, we simulate an artificial priority queue, using a priority function

$$
\text{Priority} = \alpha U + (1- \alpha) I
$$ {#eq-prio}

where $U, I, \alpha$ are "urgency," "importance," and the urgency weight, respectively.

Our first step is to run a discrete-event simulation (DES) to uncover the relationship between $\alpha$ , probability of survival, and burstiness measures. DES is a technique for modelling stochastic, dynamic and discretely evolving systems. In this framework, consider:

1.  An individual who is able to finish any job

2.  A ready supply of jobs with no prospect of any shortages

3.  Jobs are allocated a priority when they arrive in the queue according to @eq-prio

4.  The time taken to complete a job is variable but independent of the task

We define the completion time for the different activities as random draws from an exponential distribution with $\lambda = 5$. Likewise, the interarrival times for tasks are from an exponential distribution of $\lambda = 5$. Each incoming task receives $U, I \sim Unif(0,1)$ and a priority according to @eq-prio. Tasks are completed in the order of priority.

Importantly, each task contains a survival rate as a function of its importance. The higher the urgency, the more likely the task is to fail (or die off). This feature internalizes the fact that urgent tasks may bring imminent negative consequences. The survival probability of a task is given by a Weibull distribution with a monomial function:

$$
\Pr (\text{survival}|\alpha = s(1+U),\beta=1) = 1 - e^{(-t/\alpha)^\beta}
$$ where $t$ is the wait time, and $s$ is a scaling parameter (in the following simulations calibrated to be 100). As such, each task contains a survival probability, which is then accumulated throughout the ask, such that the cumulative survival rate of later tasks are smaller than those of earlier ones.

By simulating this queue, we can graph a heatmap with the relationship of the weight of urgency on the priority function ($\alpha$) and cumulative survival rate:

```{r}
#| label: simmer-simulation
#| fig-cap: "Probability of Survival by Urgency Weight"

urgency_survival_grid <- read_csv("urgency-survival-grid.csv") %>%
  rename(time = mean_time) %>%
  filter(!is.na(mean_survival)) %>%
  filter(time < 20) # Anything greater is close to 0

levelplot(mean_survival ~ time * weight, urgency_survival_grid, 
          panel = panel.2dsmoother(n = 200))

```

@simmer-simulation shows that those with higher urgency weights are more likely to survive longer in the queue. <!--# How does SV relate to the weight? (tradeoff: important tasks vs. risk of survival) -->

Further, with interevent times, we can calculate the burstiness and memory as a function of urgency weights.

```{r}
#| label: simmer-burstiness
#| fig-cap: "Burstiness by Urgency Weight"

urgency_burstiness_grid <- read_csv("urgency-burstiness-grid.csv") 

library(viridis)
plt_bursty <-
  ggplot(urgency_burstiness_grid,
         aes(x = M, y = B)) +
  geom_jitter(aes(color = weights),
              size = 1,
              alpha = 0.8) +
  labs(color = "Urgency Weight") +
  scale_color_viridis(option = "D")

plt_bursty +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0)

```

@simmer-burstiness shows that the surviving tasks are indeed around the area of bursty phenomena as reported in the literature. However, there does not seem to be a relationship between urgency weighting and burstiness.

## Relating to Poverty {#sec-poverty}

The literature suggests that the persistence of generational poverty is propagated by countless environmental, physical and psychological factors known as "Poverty Traps" (Barrett et al 2016). Though the dynamics of these negative feedback loops are heavily researched, the commonly invoked mechanism suggests that these traps exist due to a country, person or economy being unable to accumulate enough capital for incomes to rise. However, more recent literature suggests that investing additional capital within an environment will not raise the long-term steady-state income level (Kraay et al 2014) - contradicting the commonly held position. Many studies also investigate whether S-shaped curves, a great predicator of temporally dynamic relationships between income and assets, are indicative of poverty traps. However, this type of analysis has frequently yielded incorrect assessments of poverty-trapping due to a proclivity for numerical errors that such an approach poses. Given the misleading nature of many theoretical approaches to characterize poverty traps, the limited strain of literature addressing this topic has recently begun veering in the direction of seeking behavioral prescriptions.

Our analysis of poverty traps will explore how self-reinforcing behaviors in priority queues represent distinct facets of poverty traps.

### Irrationality of Time Allocation

A particular facet of "poverty traps" can be attributed to the inefficiency of daily task management. Individuals are described as making a sequence of decisions and bargains in order to reduce the strain of their daily demands and roles in a process similar to that of an economic decision: Allocating limited resources such as energy, time, emotions, and goods (Goode, 1960) in different ways. For individuals who belonging to a higher socioeconomic class (SEC), delegating these roles becomes far easier, and the bandwidth to tackle other tasks increases significantly. The converse is true however as well: the poorer one is, the more uncontrolled demand there is on both time and money.

### Role Strain

Role strain, the "effort" dimension or subjective value individuals attribute to completing arbitrary tasks, is observed to be higher among the poor (Hopper et al 2020). Formulated in 1960 by Columbia sociologist William Goode, this theory argues that individuals cannot meet all role demands, and through a series of role decisions and bargains, they strive to adjust these demands. In this process, they allocate scarce resources\--role energies, time, emotions, goods\--among the role obligations they have to fulfill. In other words, individuals seek to minimize their perceived role strain (the cognitive and emotional effort of carrying out their duties).

Adding tasks to already demanding circumstances will only add to role strain, while adding an extra participant to assist with tasks will instead diminish the strain. A hypothesized cause of poverty traps is the proclivity for the impoverished to over-commit to high urgency tasks, thereby neglecting or under-commiting to high effort or high importance tasks. Goode concurs with this assessment, claiming that conflicts among task choices typically arise through role conflict (when two roles have demands which are mutually exclusive) or role overload (when an individual is too resource scarce to meet the demands of multiple roles). The latter holds more weight in our research goals, an assertion we further investigate.

### Scarcity

Notably, time and resource scarcity are positively correlated with socioeconomic class. Essentially, a paucity of goods is intrinsic to individuals who are poor. The behavioral patterns which emerge as a result, align with the expected results from Role Strain literature. In countless replication studies, it has been determined that scarcity yields an increase in focus during task completion. The most prominent replication exercise conducted five experiments in which different resources were withheld in order to induce a scarcity complex (Shah et al 2012). In each game, participants were awarded a differing number of "guesses," or "attempts" as a source of currency in order to simulate either resource abundance or scarcity across players. It was observed that when players were more resource scarce, focus on the task increased as a response to the limited opportunities for success that were available. Although the efficiency and accuracy at tasks completion improved for such players, a byproduct was an increase in stress as well. This same outcome has been confirmed in alternative literature (de Bruijn et al 2021), when it was determined that for individuals who have less resources, their daily tasks become increasingly taxing and necessitate more attention (AdamkoviÄ et al 2022).

### Cognitive Load

The cognitive fatigue that such tasks cause individuals to endure can become overwhelming. Countless literature suggests that in the aftermath of highly strenuous tasks caused by induced scarcity, attentional shifts become more commonplace. However, other studies claim that there may not be enough information to thoroughly arrive at this conclusion. Nonetheless, nearly all of the current literature converges to the understanding that poverty inadvertently results in overborrowing of resources -- particularly of those that are scarce. This phenomenon is exemplified with debt -- low SES individuals with a lower credit score are at higher risk for requesting loans.

## Relating to Anxiety

In this study, we propose that the behavioral deviations brought by overweighting urgency are mediated by affect. Specifically, it is through stress or anxiety that these shifts occur. Previously, Morris and Coley (2004, <https://psycnet.apa.org/record/2004-18349-003>) examined correlates of role strain among mothers. They find a variety of factors that increase the "strain" in this population. By extension, we propose three channels though which role strain may increase the urgency bias, as follows:

```{mermaid}
flowchart Role Strain
  A[Income] --> B{Role Strain}
  C[Social Support] --> B
  D[Education] --> B
  E[Health Insurance] --> B
  F[Gov't Welfare] --> B
  B --> G(Subjective Ratings of Strain)
  B --> H(Stress and Anxiety)
  B --> I(Urgency Bias)
  G --> I
  H --> I
```

# The Experiment

In practical applications, we can imagine an icecream parlor as a paragon example of Priority Queuing in which tasks require are imbued with varying degrees of effort, urgency and value. Effort is a tangible variable and can be measured by complexity of icecream created: additional toppings, or additional flavors. We imagine importance as the monetary value, or utility, reaped from completing a task. Urgency can be measured by customers who are impatient. We imagine such participants as having a finite time until they leave the queue and both the value and effort variable associated with their task becomes dismissed.

## Structure

Our experiment, created on Pavlovia, simulates the outlined scenario in an online format. We construct a 2-task queue which indicates what type of "order" or task an agent must attend to: (high/low) urgency, (high/low) effort, (high/low) importance. High importance is visually represented by two dollar signs, which low importance is represented by one dollar sign. Likewise, two heart represent high urgency, and one heart represents low urgency. Finally, the effort variable is characterized by the number of "clicks" and the likelihood to "squint" - real effort costs for participants - a specific task requires. This generally occurs when specific sprinkle/scoop pairings are presented. Note that the nomenclature for identifying these tasks has a three symbol standard, where the first symbol represents the value of a task (H or L for high or low), the second symbol represents the effort (H or L) and the final symbol represents the urgency (H or 0 for high or non-urgent).

![The top of the display is the 2-task queue, the bottom of the display are the clicks participants must make to successfully complete a task.](images/2ice.png){#img-choice}

For every iteration of the game that a high urgency task is unattended to, the value of the task decreases - at which point the participant leaves the queue if the value reaches no dollar signs.

To incentivize players in the game, we offer a \$1.00\$ prize which is contingent on the total number of dollar-signs, total value, players accumulate throughout the game. A counter is positioned on the display to indicate progress. Likewise, we perform 20 iterations of this experiment: sufficient time to gauge patterns within participant behavior. An important feature of this experiment is that participants can attend to tasks in any order, not necessarily left to right, which effectively should indicate how they prioritize all three of these factors. In general, our rudimentary experimental design appears as follows:

![The figure above represents the basic interface programmed for the game. Participants have a queue at the top of the game, and a series of icons they must press in order to complete a task. The hearts and dollar signs on a task represent the importance and urgency of the task and the money counter at the bottom of the screen indicates the accumulated wealth of the participant.](images/totalice.jpeg){#img-exp}

Although we do not gather inter-event data from this game <!--# can we use the RTs to calculate tau? -->, players are unaware of this feature. We expect that many participants rush and become stressed - traits of induced scarcity. To measure relative SES and relative anxiety, we use a post and intro survey to determine a relative deprivation index and also a set of STAI.

We should note that specific task situations require minimal cognition. If an individual is presented with a high value, high urgency task then they will always choose this this task. However, if an individual is presented a high importance, low urgency task and a high urgency, low importance task - it is not entirely apparent what the optimal course of action for future outcomes may be, and more so what choices individuals will make. Using R, we capture these results and output baseline charts of what the choice distribution looks like.

### Simulation

We devised a theoretical model for how we expect rational decision-making to manifest in our priority queuing experiment such that frequency of deviations from rationality can be monitored. To formalize the optimal, or rational, set of behavior for maximizing rewards over long-term exposure in a queue, we developed a closed-form model. We hypothesize that this decision-making process can be summarized by a simple subjective-value function, where every individual determines the priority or preference $\succeq_q$ by the subjective value utility representation. We thus developed a stochastic model, as follows:

$$
\begin{eqnarray}
    V(s) &=& \max\limits_{a}(r(s,a) + \gamma\sum\limits_{S}P(s,a, s')V(s'))\\
    S &=& \{HHU \times HHU, HHU \times HH0, LHU \times LHU, â¦\}\\
    r(a,s) &=& i_a - c_a\\
    R &=& {r(HHU, HHV \times HHU) = 1 - c_H, â¦}\\
    V(s_{final}) &=& \max\limits_{a}(r(s_{final}, a))\\
\end{eqnarray}
$$ {#eq-model}

In @eq-model the agent calculates the value of state $s' \in S$ (which is the next state, i.e., next period). $i$ is the reward (in dollars) for completing a task, $c_i$ is the effort cost (estimated in dollar unites), and $r(a,s)$ is the net payoff of completing action $a$ at state $s$. $P$ is the probability of transitioning from state $s$ to another state $(s')$, given that our agents takes action $a$ in state $s$. $V(s')$ is the value of that new state. We subsequently sum these results over all the possible future states. Note we use $\gamma$ as our discount factor. In our normative model, we set the discounting factor to be 1. Note that this model only has two cost parameters $c=\{c_L, c_H\}$.

We build upon this by then generating an optimal protocol for identifying the best set of choices at any stage for maximizing long-term outcomes. Using dynamic programming, a recursive method of taking the outcomes of subgames and applying it to the entire scheme, we identify which tasks should be chosen given stage of the 2-task queue. Given this framework, we can graph the ratio of urgent choices at each given state, as a function of the cost parameter $c_H$ and the discount factor (here $c_L = 0$ for all cases).

```{r}
#| label: dyn-pro-simulation-results
#| fig-cap: "Percentage of Urgent Choices"

urgency_ratio_grid <- read_csv("urgency-ratio-grid.csv") 
urgency_ratio_grid <- urgency_ratio_grid %>%
  rename(gamma = `...1`) %>%
  pivot_longer(!gamma, names_to = "cost", values_to = "urgent") %>%
  mutate(cost = as.numeric(cost))

levelplot(urgent ~ cost * gamma, urgency_ratio_grid, 
          panel = panel.levelplot.points, cex = 0
    ) + 
    latticeExtra::layer_(panel.2dsmoother(..., n = 200))

```

@dyn-pro-simulation-results shows that the optimal percentage of urgent choices varies by around 0.10, with lower probabilities centered around low cost parameters and high discount factors.

## Data

We administered our experiment over both Mturk and Prolific. In our MTurk administration, we acquired N = 22 participants, and via Prolific, N = 53. Here, we treat these samples as independent replications.

```{r}
#| label: data-wrangling-pilot
joint_pilot %>%
  group_by(platform) %>%
  mutate(
    high_income = 
      case_when(as.numeric(income) >= median(as.numeric(income), na.rm = TRUE) ~ 1,
                as.numeric(income)  < median(as.numeric(income), na.rm = TRUE) ~ 0),
    high_anxiety = 
      case_when(stai >= median(stai, na.rm = TRUE) ~ 1,
                stai  < median(stai, na.rm = TRUE) ~ 0)
  ) %>%
  mutate(
    high_income = factor(high_income,
                         ordered = TRUE,
                         labels = c("Low Income", "High Income")),
    high_anxiety = factor(high_anxiety,
                          ordered = TRUE,
                          labels = c("Low Anxiety", "High Anxiety"))
  ) %>%
  # Extract the indeces which differ for each variable: 
  mutate(
    heart = (u1 != u2), # <-- index @ which diff urgency
    dollar = (i1 != i2), # <-- index @ which diff importance
    effort = (e1 != e2), # <-- index @ which diff effort
    left_choice = (choice == "L"), # < --- index when left is chosen
    right_choice = (choice == "R"), # <--- index when right is chosen
    split = ((u1 != u2) & (i1 != i2) & (u1 != i1)), # <--- "hard" decision
    ui_leftsplit = left_choice*split, # <--- "hard" decision AND left is chosen
    ui_rightsplit = right_choice*split
  ) %>% 
  # Find the means of the values:
  mutate(
    huli = case_when(split == 0 ~ NA,
                     ui_leftsplit  == 1 & u1 == 2 ~ 1,
                     ui_rightsplit == 1 & u2 == 2 ~ 1,
                     ui_leftsplit  == 1 & u1 == 1 ~ 0,
                     ui_rightsplit == 1 & u2 == 1 ~ 0),
    luhi = case_when(split == 0 ~ NA,
                     ui_leftsplit  == 1 & i1 == 2 ~ 1,
                     ui_rightsplit == 1 & i2 == 2 ~ 1,
                     ui_leftsplit  == 1 & i1 == 1 ~ 0,
                     ui_rightsplit == 1 & i2 == 1 ~ 0)
  ) -> joint_pilot
```

### Demographics

For both Mturk and Prolific, we distinguish high-low income and high-low anxiety by a median split. The high income mean is about 70k, while the low income mean is around 30k $(p<0.001)$. Likewise, the high anxiety mean score is 56.29, while low anxiety mean score is 37.17 $(p<0.001)$.

+------------------+------------+----------------------+----------------------+----------------------+
| MTurk\           | Mean       | Median               | Min                  | Max                  |
| N=22 x 20 trials |            |                      |                      |                      |
+:=================+:===========+:=====================+:=====================+:=====================+
| Age              | 34.53      | 33                   | 19                   | 50                   |
+------------------+------------+----------------------+----------------------+----------------------+
| Income           |            | \$40,000 to \$49,999 | Less than \$10,000   | \$90,000 to \$99,999 |
+------------------+------------+----------------------+----------------------+----------------------+
| Education        |            | Bachelor's degree    | High school graduate | Master's degree      |
+------------------+------------+----------------------+----------------------+----------------------+
| STAI             | 44.21      | 47                   | 20                   | 67                   |
+------------------+------------+----------------------+----------------------+----------------------+

+------------------+------------+----------------------+----------------------+----------------------+
| Prolific\        | Mean       | Median               | Min                  | Max                  |
| N=53 x 20 trials |            |                      |                      |                      |
+:=================+:===========+:=====================+:=====================+:=====================+
| Age              | 30.13      | 28                   | 18                   | 75                   |
+------------------+------------+----------------------+----------------------+----------------------+
| Income           |            | \$40,000 to \$49,999 | Less than \$10,000   | \$90,000 to \$99,999 |
+------------------+------------+----------------------+----------------------+----------------------+
| Education        |            | Bachelor's degree    | High school graduate | Master's degree      |
+------------------+------------+----------------------+----------------------+----------------------+
| STAI             | 45.04      | 45                   | 20                   | 68                   |
+------------------+------------+----------------------+----------------------+----------------------+

: The two samples display similar demographics

### Measures of Poverty

Although income-based measures have long been the standard for assessing poverty, studies have since determined that purchasing power and standard of living are not accurately measured by such a metric. Rather, relative deprivation and state-trait anxiety measures are often more accurate empirical measurements of an individual's socioeconomic status (SES).

#### Self-Reported Income

Though the simplest measure to collect, self-reported incomes has a number of limitations. Participants may have the incentive not to divulge their true income levels. This bias can occur in both directions depending on the context and demand effects (e.g., a participant may want to appear richer or poorer than they actually are). Further, because exact income, when combined with other demographic measures, could potentially be used to identify participants, it is customary to elicit income brackets (vs. absolute income levels). This limitation may also reduce power and granularity in the our analysis.

#### ZIP code Measures

To remove measurement error from self-reported income, we use median income and poverty measures by the zip code of each participant, as provided by Qualtrics (which collects IP addresses of each participant).

Our poverty measures are from the American Community Survey (ACS). The Census Bureau uses a set of dollar value thresholds that vary by family size and composition to determine who is in poverty, with thresholds (income cutoffs) arranged in a two-dimensional matrix (income and family size, from one person to nine or more people).

Our median income data is based on the distribution of the total number of households and families including those with no income.

For more information, check <https://www2.census.gov/programs-surveys/acs/tech_docs/subject_definitions/2021_ACSSubjectDefinitions.pdf>

#### Â¶Composite Measure (PCA)

Given that poverty is a multidimensional variable, we attempt to capture it through a series of questions (see @questionnaires for details). We created several questions about physical, mental, and monetary deprivation. We then summarized these results in a PCA. We use only the top two components, as per the Scree plot:

```{r}
#| fig-cap: pca-measure

```

##### "Dimension 1" top 5 contributors:

(Higher dimension values = higher "depression")

1.  How would you rate your mental health over the past 4 weeks? (1-5)
2.  Please select Yes or No for each of the following questions (YES = 1, NO = 2).
    a.  I am happy about achieving something good in the last 4 weeks.
    b.  Depressed and unhappy.
3.  Over the past 2 weeks, how often have you felt down, depressed, or hopeless?
4.  Over the past 2 weeks, how often have you felt little interest or pleasure in doing things?

##### "Dimension 2" top 5 contributors:

(Higher dim. values = higher "concern for mental health")

1.  How important are the following:
    a.  How you feel about yourself
    b.  Your mental state of being
    c.  Your physical state of being
2.  How important are the following aspects of your life?
    a.  I need to be mentally healthy
    b.  Having a good house to stay in

Note: We included other deprivation questions related to physical/monetary deprivation. However, those questions did not yield sufficient variance.

## Results

### Choice

#### Â§Trade-off Decisions

When we compare an individual's choice of "challenging" tasks: decisions between high urgency - low importance or low importance - high urgency tasks, we observe that high anxiety and low income individuals more often choose high urgency tasks over high value tasks, which corroborates the current non-quantitative literature as can be observed in the plots below.

```{r trade-off-results}
#| label: trade-off-results
#| fig-cap: "Choice proportions by income and anxiety"
#| fig-subcap:
#|   - "Income"
#|   - "Anxiety"
#| layout-ncol: 2
#| column: page-right

joint_pilot %>%
  group_by(platform, high_income) %>%
  filter(!is.na(high_income)) %>%
  summarise(choose_urg = sum(huli, na.rm = TRUE),
            total = sum(!is.na(huli)),
            mean_huli  = mean(huli, na.rm = TRUE)) %>%
  mutate(se = sqrt((mean_huli * (1-mean_huli)) / total)) %>%
  
  ggplot(aes(platform, mean_huli, fill = high_income),
         alpha=0.7) + 
  geom_col(position = "dodge") +
  labs(title="Choice Frequency of Urgent vs Non-urgent Tasks",
       x ="Platform",
       y = "Frequency of urgent/not important choices",
       fill = "") +
  scale_fill_brewer() +
  theme_minimal() +
  geom_errorbar(aes(ymin = mean_huli - se, ymax = mean_huli + se),
                width=0.4,
                alpha=0.9,
                position = position_dodge(0.9)) + 
  geom_hline(yintercept=0.5, linetype=2)


joint_pilot %>%
  group_by(platform, high_anxiety) %>%
  filter(!is.na(high_anxiety)) %>%
  summarise(choose_urg = sum(huli, na.rm = TRUE),
            total = sum(!is.na(huli)),
            mean_huli  = mean(huli, na.rm = TRUE)) %>%
  mutate(se = sqrt((mean_huli * (1-mean_huli)) / total)) %>%
  
  ggplot(aes(platform, mean_huli, fill = high_income),
         alpha=0.7) + 
  geom_col(position = "dodge") +
  labs(title="Choice Frequency of Urgent vs Non-urgent Tasks",
       x ="Platform",
       y = "Frequency of urgent/not important choices",
       fill = "") +
  scale_fill_brewer() +
  theme_minimal() +
  geom_errorbar(aes(ymin = mean_huli - se, ymax = mean_huli + se),
                width=0.4,
                alpha=0.9,
                position = position_dodge(0.9)) + 
  geom_hline(yintercept=0.5, linetype=2)

```

Accuracy in trade-off decisions as a function of income and anxiety

Multiple regression models to test the same hypotheses with different measures of income.

```{r}
library(tidymodels)
joint_pilot %>%
  mutate(huli = factor(huli,
                       ordered = FALSE,
                       levels = c(1,0),
                       labels = c("High Urgency, Low Importance",
                                  "Low Urgency, High Importance"))) %>%
  prop_test(formula = huli ~ high_income,
            alternative = "greater",
            order = c("Low Income", "High Income")) -> pooled_income_test

joint_pilot %>%
  mutate(huli = factor(huli,
                       ordered = FALSE,
                       levels = c(1,0),
                       labels = c("High Urgency, Low Importance",
                                  "Low Urgency, High Importance"))) %>%
  prop_test(formula = huli ~ high_anxiety,
            alternative = "less",
            order = c("Low Anxiety", "High Anxiety")) -> pooled_anxiety_test

joint_pilot %>%
  filter(high_income == "Low Income") %>%
  mutate(huli = factor(huli,
                       ordered = FALSE,
                       levels = c(1,0),
                       labels = c("High Urgency, Low Importance",
                                  "Low Urgency, High Importance"))) %>%
  prop_test(formula = huli ~ NULL,
            success = "High Urgency, Low Importance",
            p = 0.5,
            alternative = "greater") -> low_income_test

joint_pilot %>%
  filter(high_anxiety == "High Anxiety") %>%
  mutate(huli = factor(huli,
                       ordered = FALSE,
                       levels = c(1,0),
                       labels = c("High Urgency, Low Importance",
                                  "Low Urgency, High Importance"))) %>%
  prop_test(formula = huli ~ NULL,
            success = "High Urgency, Low Importance",
            p = 0.5,
            alternative = "greater") -> high_anxiety_test

```

Percent of urgent choices (vs. important) as a function of income and anxiety.

::: callout-note
## Key Result

Individuals in the low income groups and in the high anxiety groups chose urgent tasks more often than important tasks. This result is not consistent for high income or low anxiety groups. Future graphs in this document will show that the difference-in-difference is not as clear.
:::

1.  

2.  Multiple regression models to test the same hypotheses with different measures of income.

3.  

```{r}
#| label: zip-code-measures

# Processing data
demograph <- qualtrics[qualtrics$PROLIFIC_PID %in% pilot1$participant,]
participants_df <- aggregate(points ~ participant, data=pilot1, FUN = mean)
demograph <- merge(y = qualtrics,
                  x = participants_df,
                  by.y = "PROLIFIC_PID",
                  by.x = "participant")
ip2zip <- read_csv("./3.2 Data/raw/ip2zip.csv")
demograph <- merge(y = demograph,
                   x = ip2zip,
                   by.y = "IPAddress",
                   by.x = "IP")
### HERE:
poverty <- read_csv("./3.2 Data/raw/PovertyStatus-ZIPcode/ACSST5Y2019.S1701_data_with_overlays_2021-12-21T174024.csv")
poverty <- poverty %>%
  select(NAME,S1701_C03_001E) %>%
  mutate(ZIP = as.numeric(str_sub(NAME, start= -5))) %>%
  select(-NAME)
demograph <- demograph %>%
  left_join(poverty, by = "ZIP")

income <- read_csv("./3.2 Data/raw/Income-ZIPcode/ACSST5Y2019.S1903_data_with_overlays_2021-11-16T180635.csv")
income <- income %>%
  select(NAME,S1903_C03_001E) %>%
  mutate(ZIP = as.numeric(str_sub(NAME, start= -5))) %>%
  select(-NAME)
demograph <- demograph %>%
  select(-"IP") %>% ## Remove IP for privacy and cleaning
  left_join(income, by = "ZIP")
rm(ip2zip, income, poverty)

```

PCA \~ Urgent Choices Analysis

It seems that when controlling for Dim 1 and 2, some effects become significant. Multicollinearity might be a problem, although correlations between STAI and PCA components are not higher than 0.6 (see correlation plots above). These results might be a result of a separation between State and Trait Anxiety. Although we only included the State questions of STAI, state and trait scores are likely correlated. PCA dimensions 1 and 2 may control for this confounding effect.

### Â¶Reaction Time

M:

### Â¶Accuracy

We also obtain a comparison of the choices participants make and the optimal, or rational, protocol we generated.

Histogram for Prolific Data. Unlike MTurk, this distribution looks normal. This score is calculated by first estimating the cost parameters for each participant and then using them with backwards induction to discover the optimal choice at each presented state.

K: explanation + histogram

```{r}
#| label: calculate-optimum

# Extracting parameter values
ShinyStan_2 <- summary(MTurk_sample_nolimit2)$summary
costs <- data.frame(matrix(nrow = 56))
costs$cL <- ShinyStan_2[c(5:60),1]
costs$cH <- ShinyStan_2[c(61:116),1]
costs <- costs[,-c(1)]

# With estimated parameters, find optimal choice
ev <- c(0,0)
value <- c(0,0,0,0)
#Tsubj <- Tsubj$n
value_lookup = model_data$value_lookup
counterpart <- model_data$counterpart
choice_best <- as.data.frame(matrix(0, 56, 40))
Tsubj <- model_data$Tsubj
state_lookup <- model_data$state_lookup
prob_weight <- model_data$prob_weight
opt_st <- model_data$opt_st
#opt_st <- opt_st[,-c(1)]
for (i in c(1:model_data$N)) {
  ev <- c(0,0)
  ch <- matrix(0, 80, Tsubj[i])
  st <- matrix(0, nrow = 52, ncol = Tsubj[i])
  
  # Declaring values for each option, lookup table (make it loop later)
  value[1] = 2 - costs[i,2];
  value[2] = 1 - costs[i,2];
  value[3] = 1 - costs[i,1];
  value[4] = 2 - costs[i,1];
  if (i %% 10 == 0){
    print(i)
  }
  for(option in c(1:80)) {
    ch[option, Tsubj[i]] = value[value_lookup[option]]
  }
  for(state in c(1:52)) {
    if (ch[state_lookup[state,1], Tsubj[i]] >= ch[state_lookup[state,2], Tsubj[i]]) {
      st[state, Tsubj[i]] = ch[state_lookup[state,1], Tsubj[i]]
    } else if (ch[state_lookup[state,1], Tsubj[i]] < ch[state_lookup[state,2], Tsubj[i]]) {
      st[state, Tsubj[i]] = ch[state_lookup[state,2], Tsubj[i]]
    }
  }
  ev[1] = ch[as.numeric(opt_st[i, Tsubj[i]]), Tsubj[i]]
  ev[2] = ch[counterpart[as.numeric(opt_st[i, Tsubj[i]])], Tsubj[i]]
  if (ev[1] >= ev[2]) {
    choice_best[i, Tsubj[i]] = 1
  } else {
    choice_best[i, Tsubj[i]] = 2
  }
  
  if (!Tsubj[i] == 1) {
    
    for (t in c(1:(Tsubj[i]-1))) {
      round_back = Tsubj[i] - t
      for(option in c(1:80)) {
        weighted_value = as.numeric((prob_weight[option,])) %*% st[,(round_back + 1)]
        ch[option, round_back] = value[value_lookup[option]] + weighted_value
      }
      for(state in c(1:52)) {
        if (ch[state_lookup[state,1], round_back] >= ch[state_lookup[state,2], round_back]) {
          st[state, round_back] = ch[state_lookup[state,1], round_back]
        } else if (ch[state_lookup[state,1], round_back] < ch[state_lookup[state,2], round_back]) {
          st[state, round_back] = ch[state_lookup[state,2], round_back]
        }
      }
    }
    for (t in c(1:(Tsubj[i]-1))) {
      ev[1] = ch[as.numeric(opt_st[i, t]), t]
      ev[2] = ch[counterpart[as.numeric(opt_st[i, t])], t]
      if (ev[1] >= ev[2]) {
        choice_best[i, t] = 1
      } else {
        choice_best[i, t] = 2
      }
    }
  }
}

#####  Accuracy of all trials
# Ignores people with less than 10 trials:
choice <- model_data$choice

choice_best <- choice_best[Tsubj > 10,c(1:36)]
choice1 <- choice[Tsubj > 10,-c(1)]
choice1[choice1 == 0] <- NA
choice2 <- choice_best
choice2[choice2 == 0] <- NA

comparison <- (choice[Tsubj > 10,] == choice_best)
comparison[is.na(choice2)] <- NA
Accuracy <- rowMeans(comparison, na.rm=T)
hist(Accuracy)
Accuracy <- cbind(Accuracy, demograph[Tsubj > 10,]$participant)

Accuracy <-merge(x = Accuracy, y = demograph, by.x = "V2", by.y = "participant")
Accuracy$Accuracy <- as.numeric(Accuracy$Accuracy)

```

```{r}
#| label: accuracy-histograms
#| fig-cap: "The figure above highlights how participants typically make optimal decisions (when given 'challenging' tasks) with a probability in between 0.5 and 0.6. This is generally what we expect to observe."



```

#### Â¶Accuracy, Anxiety, and Income

1.  Accuracy in trade-off decisions as a function of income and anxiety

Finally, we compare for each participant, how many of their choices are identical to a model that observed the full horizon for the Markov decision process. Each participant is a dot, y axis is the peerage of correct choices in the experiment while x axis is the STAI (anxiety measure). Generally the results are very similar to figure 1.5.

```{r}
#| label: accuracy-results
#| fig-cap: "Accuracy by Income and Anxiety"
#| fig-subcap:
#|   - "Mturk, Income"
#|   - "Mturk, Anxiety"
#|   - "Prolific, Income"
#|   - "Prolific, Anxiety"
#| layout-ncol: 2
#| column: page-right



```

It appears that Accuracy does not vary across income and anxiety levels with these measures. Some possibilities of this fact might include that, since time discounting here is virtually 0, leaving the important task behind will not affect participants as much (depending, of course, of the estimated cost parameters).

# Future Directions

## Â¶Power Analysis

### Behavioral Measures

IRB

### Physiological Measures

"Sartorial Symbols of Social Class Elicit Class-Consistent Behavioral and Physiological Responses: A Dyadic Approach" (Kraus et al 2014)

Goal of paper: Determine the psychological response among participants paired across socio-economic groups.

N = 128, 64 dyads (20 upper class, 20 lower class, 24 neutral condition)

DV = HRV (Heart Rate Variability) reactivity differences between high vs. low class

Prior = Upper-class = -0.69, se = 0.31; Lower-class = 0.04, se = 0.31

```{r}

# HVC across SES
Estimated sample sizes for a two-sample means test
t test assuming sd1 = sd2 = sd
HO: m2 = m1 versus Ha: m2 != m1
Study parameters:
alpha = 0.0500
power = 0.8000
delta = -0.7300
m1 = 0.0400
m2 = -0.6900
sd = 1.3860
Estimated sample sizes:
N=
N per group =
116
58
```

## Â¶Redesign

1.  A clock on the screen will display how long you have until that task, or the customer, decides to leave the line.

2.  This will mean you can no longer make money for that task.

3.  In the first round of this experiment, you will experience monetary "shocks." This means that at random times, some money might be taken from your total accumulated reward with no warning.

4.  In the second round of this experiment, the rate at which orders arrive will be doubled at random times. However, there will be no monetary shocks.

5.  Queue length is not fixed.

\- Skin conductance, Cortisol, Eye tracking

### Â¶Probabilistic Choice

As seen in @accuracy-results,

IRB

### Perceived Effort

Stress has been suggested to increase perceived effort (e.g., the study will investigate whether it's more tiring to complete a task when stressed), leading low-income individuals to be less likely to take up welfare programs, regardless of eligibility (Bertrand et al., 2006; Hernanz et al., 2004).

In our model, cost can include a subjective component. Instead of counting it as simply the number of clicks, we can assume that the agent may dread or fantasize about action $a$. As such, someone who likes the task might not be bothered by doing it so the cost may be very close to zero (or negative). And vice versa for someone who hates it. Similarly, we can account for substituting *subjective* probability for objective probability. That is, people might distort the actual probability of future events.

### Flexible Queue Size

IRB

### Comprehensive Measures of Poverty

IRB

As seen in @pca-measure, Revisit questionnaire to address variance problems. One possibility is to expand our sample to lower-income people. Another option is to try other questions that would elicit more variance.

Refer to @questionnaires for our current iteration of comprehensive measurements.

## Population

In Los Angeles, 18% of residents worry every day or almost every day about paying their bills, and 23% worry about job loss (Baldassare et al., 2021). This constant mindset of worry likely affects the livelihood of these citizens. In a recent PPIC survey, about "one in five Californians report that they or someone in their household has cut back on food (21%), put off seeing a doctor or purchasing medicine to save money (18%), been unable to pay a monthly bill (17%), or had difficulty paying the rent or mortgage (17%) in the last 12 months" (Baldassare et al., 2021).

### Online Sample

Pros and cons.

### Homeless

M: Look Sera's studies

IRB: Justification

### Other Low SES Cohorts

M: Proposal

# Appendix {.unnumbered}

## The Eisenhower Matrix {#app-eisenhower}

"Who can define for us with accuracy the difference between the long and short term! Especially whenever our affairs seem to be in crisis, we are almost compelled to give our first attention to the urgent present rather than to the important future."

\-\-- Dwight D. Eisenhower, 1961 address to the Century Association

[![Source: https://todoist.com/productivity-methods/eisenhower-matrix](https://images.ctfassets.net/dm4oa8qtogq0/6Z0iRvBwjXMLrd28BUzTW4/a3b7838c90a65b26a2edb6814e7430dd/eisenhower-matrix.png){width="5.36in"}](https://todoist.com/productivity-methods/eisenhower-matrix)

## Fantasy and Dread Model {#app-fantasy-dread}

$$
\begin{aligned}
& V\left(s^{\prime}\right)=r(a)-c(a)+\gamma \sum_s P\left(a, s^{\prime}, s^{\prime \prime}\right) V\left(s^{\prime \prime}\right) \\
& \mathrm{SV}\left(\mathrm{s}^{\prime}\right)=\mathrm{r}(\mathrm{a})-e\left(a \mid z_e\right)+\gamma \sum_s S P(\cdot) V\left(s^{\prime}\right) \\
& e\left(a \mid z_e\right)=z_e(c(e)) \\
& \mathrm{SP}\left(a, s^{\prime}, s^{\prime \prime} \mid z_p\right)=z_p\left(P\left(a, s^{\prime}, s^{\prime \prime}\right)\right)
\end{aligned}
$$

where $z_e, z_p$ correspond to fantasy/dread and a sense of urgency, respectively. One possible

P() depends on whether unattended task "decays"

::: {#fig-prob-weighting layout-ncol="2"}
![Probability Weighting](images/prob-weighting.png){#fig-prob-weight}

![Probability Example](images/sense-urgency-illustration.png){#fig-sense-urg}

Example of Subjective Probabilities
:::

## Illustration: Priority Queuing

::: {#fig-prioq layout-ncol="2"}
A list can contain an arbitrary number of tasks and the priority of each task is an integer drawn from some distribution. The tasks are set to arrive with the rate $\lambda$ following a Poisson dynamics with exponential arrival time distribution and they are executed with rate $\mu$ by always choosing the one with the highest priority.

![Sample Queue](images/prioq-illustration.png){width="3.2in"}
:::

## Â¶Dynamic Programming Simulation

## Â¶Bayesian Model for Cost Estimation

### Model

### Estimation

## Code

### Priority Queuing Simulation

```{r pri-q-simulation, eval=FALSE, include=TRUE}

library(simmer)
library(simmer.plot)
library(tidyverse)
library(latex2exp)
#library(dplyr)
#library(data.table)
set.seed(1234)

# Parameters
lambda <- 5
mu <- 5
t_simul <- 150 #Calibrated to optimize simulation time (prob_continue â 0)
n_simul <- 2
scale = 100 # The scale on alpha to transfer runif(0,1) to something with more variance

weights <- seq(0, 1, by = 0.001)
arri <- list()
attr <- list()
total <- list()

# Simulations with simmer
for (w in 1:length(weights)) {
  env <- simmer("poverty")
  
  person <- trajectory("Poor's Trajectory") %>%
    set_attribute(keys = "urgency", function()
      runif(1, 0, 1)) %>%
    set_attribute(keys = "importance", function()
      runif(1, 0, 1)) %>%
    set_attribute(keys = "weight", weights[w]) %>%
    set_prioritization(function() {
      prio <-
        10000 * (
          get_attribute(env, "weight") * get_attribute(env, "urgency") +
            (1 - get_attribute(env, "weight")) * get_attribute(env, "importance")
        )
      c(prio, NA, NA)
    }) %>%
    # log_(function() {
    #   paste("Priority is: ", get_prioritization(env)[1])
    # }) %>%
    seize("person", amount = 1) %>%
    timeout(function()
      rexp(1, mu)) %>%
    release("person", amount = 1)
  
  env <-
    simmer() %>%
    add_resource("person", capacity = 1) %>%
    add_generator("Task", person, function()
      rexp(1, lambda), mon = 2)
  
  # env %>% run(until = t_simul)
  
  envs <- lapply(1:n_simul, function(i) {
    env %>%
      run(until = t_simul) %>%
      wrap()
  })
  
  ## Change Variables before
  arri[[w]] <- get_mon_arrivals(envs, ongoing = TRUE)
  attr[[w]] <- get_mon_attributes(envs)
  
  # Merge
  total[[w]] <-
    inner_join(arri[[w]], attr[[w]][attr[[w]]$key == "urgency", c(2, 4, 5)], by = c("name", "replication"))
  total[[w]] <-
    total[[w]] %>%
    rename(urgency = value)
  total[[w]] <-
    total[[w]][order(total[[w]]$replication, total[[w]]$start_time, decreasing = FALSE), ]
  row.names(total[[w]]) <- NULL
  total[[w]] <-
    inner_join(total[[w]], attr[[w]][attr[[w]]$key == "importance", c(2, 4, 5)], by = c("name", "replication"))
  total[[w]] <-
    total[[w]] %>%
    rename(importance = value)
  total[[w]] <-
    total[[w]][order(total[[w]]$replication, total[[w]]$start_time, decreasing = FALSE), ]
  row.names(total[[w]]) <- NULL
  total[[w]]$waiting_time <-
    total[[w]]$end_time - total[[w]]$start_time - total[[w]]$activity_time

    # Take care of precision problems yielding negative wait times.
  if (nrow(total[[w]][total[[w]]$waiting_time < 0 &
                      !is.na(total[[w]]$waiting_time), ]) > 0) {
    total[[w]][total[[w]]$waiting_time < 0 &
                 !is.na(total[[w]]$waiting_time), ]$waiting_time <- 0
  }

  #Weibull distribution with monomial function
  alpha = (scale - scale * as.numeric(total[[w]]$urgency)) #for now
  beta = 1
  total[[w]]$prob_fail <-
    1 - exp(-1 * (total[[w]]$waiting_time / alpha) ^ (beta))

  # Get probability of survival
  total[[w]] <- total[[w]] %>%
    mutate(prob_continue = (1 - prob_fail)) %>%
    group_by(replication) %>%
    mutate(prob_continue = lag(cumprod(prob_continue), k=1, default=1))

  cat('Simulation', w, 'of', length(weights), '\n')
}

sigma_list <- lapply(X = total, function(X)   sd(X[!is.na(X$waiting_time), ]$waiting_time))
mu_list    <- lapply(X = total, function(X) mean(X[!is.na(X$waiting_time), ]$waiting_time))
lag_time <- lapply(X = total, function(X) X %>%
                     filter(!is.na(waiting_time)) %>%
                     group_by(replication) %>%
                     mutate(waiting_time_lag = lag(waiting_time, default = 0)) %>%
                     dplyr::select(waiting_time, waiting_time_lag) %>%
                     cor())
memory   <- lapply(X = lag_time, function(X) X[2,3])


# Burstiness -------------------------------------------------------------
burstiness <- data.frame(weights) %>%
  mutate(sigma = unlist(sigma_list)) %>%
  mutate(mu = unlist(mu_list)) %>%
  mutate(B = (sigma - mu) / (sigma + mu)) %>%
  mutate(M = unlist(memory))

# Probability of Survival -------------------------------------------------
prob_survival <- lapply(X = total,
                        function(X) X %>%
                          group_by(name) %>%
                          summarise(mean_survival = mean(prob_continue),
                                    mean_time = mean(end_time)) %>%
                          rename(task = name) %>%
                          mutate(task = readr::parse_number(task))
                          )
names(prob_survival) <- weights
survival <- prob_survival %>%
  bind_rows(.id = "weight")

```

### Dynamic Programming Simulation

```{r dyn-pro-simulation, eval=FALSE, include=TRUE}

library(stringi)
library(stringr)

#Importance
imp <- c(1,2)
#Cost
cost1 = 0
cost2_grid <- seq(from = 0.1, to = 0.9, by = 0.05)
#Discounting
gamma_grid <- seq(from = 0.8, to = 1, by = 0.025)
#Choices
tasks <- c("HHU",
            "HH0",
            "HLU",
            "HL0",
            "LHU",
            "LH0",
            "LLU",
            "LL0")
tasks <- as.data.frame(tasks)
# Urgency Ratios
ratios <- c()

for (gamma in gamma_grid) {
  print(gamma)
  for (cost2 in cost2_grid) {
    # Add values
    tasks$value  <- c(imp[2]-cost2,
                      imp[2]-cost2,
                      imp[2]-cost1,
                      imp[2]-cost1,
                      imp[1]-cost2,
                      imp[1]-cost2,
                      imp[1]-cost1,
                      imp[1]-cost1)
    #Look up values
    r_choice <- tasks$value
    names(r_choice) <- tasks$tasks
    ## Use unname(r_choice["HH0"]) for example
    
    # States
    S <- as.data.frame(t(combn(tasks$tasks,2)))
    for (chr in tasks$tasks) {
      # Add states with two of the same task
      S <- rbind(S,c(chr,chr))
      # Add states with only one task transition to one task
      S <- rbind(S,c(chr,NA))
      # Add states with only one task transitioning to two tasks
      S <- rbind(S,c(chr,"TR"))
    }
    
    #Choice|States
    ## 1st column is choice.
    ## 1st+2nd columns are the state
    ch <- expand.grid(tasks$tasks,tasks$tasks)
    # Fix factor levels for next step
    levels(ch$Var2)
    levels(ch$Var2) = c("HHU", "HH0", "HLU", "HL0", "LHU", "LH0", "LLU", "LL0", "TR")
    for (chr in tasks$tasks) {
      ch <- rbind(ch,c(chr,NA))
      ch <- rbind(ch,c(chr,"TR"))
    }
    
    # Transition Probabilities
    choices <- paste(ch$Var1,ch$Var2,sep = "x")
    states <- paste(S$V1,S$V2,sep = "x")
    transitions <- expand.grid(choices,states)
    # Remaining task (last 3 characters) is in the next state, but no NA
    transitions$prob <- NA
    ## Non-urgent tasks:
    ## Note this is also transitioning to NA and TR states. Fix it in line 91 and __
    transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$prob <- 
      str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var2,
                 pattern = str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var1,-3))
    ## Urgent tasks: 
    ### If HXU -> LXU
    transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                  str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$prob <-
      str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                     str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var2,
                 pattern = paste("L",
                                 str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                                       str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var1,
                                         start = -2,
                                         end = -2),"U",sep = ""))
    ### If LXU -> NA
    ## first change all transitions to false
    transitions[str_detect(str = str_sub(transitions$Var2,-3), pattern = "NA"),]$prob <- FALSE
    # Now calculate other transitions
    transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                  str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$prob <-
      str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                     str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$Var2,
                 pattern = "NA")
    
    ## Single task with transition to single task:
    transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$prob <-
      str_detect(str = transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$Var2,
                 pattern = "TR")
    ## Single task with transition to single task:
    transitions[str_detect(str = transitions$Var1, pattern = "TR"),]$prob <- 0
    
    #Transforming them into probabilities by dividing by 8
    transitions$prob <- as.numeric(transitions$prob)
    transitions$prob <- transitions$prob / 8
    
    ## Single task with transition to single task:
    transitions[str_detect(str = transitions$Var1, pattern = "TR") &
                  str_length(transitions$Var2) == 7,]$prob <- 1/36
    
    ##################################################
    ################# Simulation
    ##################################################
    
    # Backwards induction
    ## Determine number of loops
    rounds = 20
    ## Create column with values of each choice for each round
    col_names <- paste("rd", c(1:rounds), sep = "")
    ch[col_names] <- NA
    ## Last round:
    ### fill in column in "ch" with the values
    for(choice in tasks$tasks) {
      ### first column is choice: find in tasks table the value of that choice
      ch[ch$Var1 == choice,rounds+2] <- tasks[tasks$tasks == choice,]$value
    }
    ### Multiply by discount factor^round
    ch[,rounds+2] <- ch[,rounds+2]*gamma^(rounds-1)
    
    
    ## Create state values for each round
    S[col_names] <- NA
    ### for each state in "S" choose the highest value in "ch" and create another column with the values
    best_option <- function(round, choice_matrix = ch, state_matrix = S, label_matrix = label){
      for(row in c(1:nrow(state_matrix))) {
        if (is.na(state_matrix[row,]$V2)) {
          state_matrix[row,round + 2] <- choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                         is.na(choice_matrix$Var2),round +2]
          label_matrix[row,round + 2] <- as.character(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 & 
                                                                      is.na(choice_matrix$Var2), 1])
        } else if (as.character(state_matrix[row,]$V2) == "TR") {
          state_matrix[row,round + 2] <- na.exclude(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                                    choice_matrix$Var2 == state_matrix[row,]$V2,round + 2])
          label_matrix[row,round + 2] <- as.character(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                                      is.na(choice_matrix$Var2), 1])
        } else {
          choice1 <- choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                     choice_matrix$Var2 == state_matrix[row,]$V2 &
                                     !is.na(choice_matrix$Var2) &
                                     as.character(choice_matrix$Var2) != "TR",round +2]
          choice2 <- choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V2 &
                                     choice_matrix$Var2 == state_matrix[row,]$V1 &
                                     !is.na(choice_matrix$Var2) &
                                     as.character(choice_matrix$Var2) != "TR",round +2]
          if (choice1 > choice2) {
            state_matrix[row,round + 2] <- choice1
            # ### save choices: change the suboptimal choices to NA
            # choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V2 &
            #                 choice_matrix$Var2 == state_matrix[row,]$V1 &
            #                 !is.na(choice_matrix$Var2) &
            #                 as.character(choice_matrix$Var2) != "TR",round +2] <- NA
            label_matrix[row,round + 2] <- as.character(na.exclude(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
                                                                        choice_matrix$Var2 == state_matrix[row,]$V2, 1]))
          } else if (choice1 < choice2) {
            state_matrix[row,round + 2] <- choice2
            # ### save choices: change the suboptimal choices to NA
            # choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V1 &
            #                 choice_matrix$Var2 == state_matrix[row,]$V2 &
            #                 !is.na(choice_matrix$Var2) &
            #                 as.character(choice_matrix$Var2) != "TR",round +2] <- NA
            label_matrix[row,round + 2] <- as.character(na.exclude(choice_matrix[choice_matrix$Var1 == state_matrix[row,]$V2 &
                                                                        choice_matrix$Var2 == state_matrix[row,]$V1, 1]))
          } else if (choice2 == choice1){
            state_matrix[row,round + 2] <- choice1
            label_matrix[row,round + 2] <- "either"
          }
        }
      }
      return(list(state_matrix,choice_matrix,label_matrix))
    }
    
    label <- S
    new_states_choices <- best_option(rounds)
    S <- new_states_choices[[1]]
    ch <- new_states_choices[[2]]
    label <- new_states_choices[[3]]
    ## second to last round to first round
    ### loop through the choices|states in "S"
    ### For each choice|state, calculate the the expected value
    
    # We need to change "transitions" for simplicity of search
    transitions$choice <- substr(transitions$Var1, 1, 3)
    transitions$non_choice <- substr(transitions$Var1, 5, 7)
    transitions[transitions$non_choice == "NA",]$non_choice <- NA
    transitions$state2_1 <- substr(transitions$Var2, 1, 3)
    transitions$state2_2 <- substr(transitions$Var2, 5, 7)
    transitions[transitions$state2_2 == "NA",]$state2_2 <- NA
    
    for(i in c(1:(rounds-1))){
      values <- S[,c(1,2,(rounds+3-i))]
      for(row in c(1:nrow(ch))){
        #### create probability vector P [44x1] from "transitions"
        if(!is.na(ch[row,2])) {
          prob_weight <- 
            transitions[ch[row,1] == transitions$choice &
                          ch[row,2] == transitions$non_choice &
                          !is.na(ch[row,2]) &
                          !is.na(transitions$non_choice),]
        } else if(is.na(ch[row,2])) {
          prob_weight <- 
            transitions[ch[row,1] == transitions$choice &
                          is.na(ch[row,2]) &
                          is.na(transitions$non_choice),]
        }
        
        #### Make sure "S" is in the same order [1x44]
        prob_weight <- merge(x = prob_weight,
                             y = values,
                             by.x = c("state2_1", "state2_2"),
                             by.y = c("V1", "V2"))
        prob_weight$weighted_value <- prob_weight$prob * values[,c(length(values))]
        ### Multiply by discount factor^round
        ch[row,rounds+2-i] <- gamma^(rounds-1-i)*tasks[tasks$tasks == ch[row,1],]$value + sum(prob_weight$weighted_value)
      }
      new_states_choices <- best_option(rounds-i)
      S <- new_states_choices[[1]]
      ch <- new_states_choices[[2]]
      label <- new_states_choices[[3]]
    }
    urgency <- apply(label[,-c(1:2)], 2, function(X) substr(X, nchar(X), nchar(X)))
    ratios <- append(ratios, table(urgency)["U"]/sum(table(urgency)))
  }
}

ratios_grid <- matrix(ratios,
                      nrow = length(gamma_grid),
                      ncol = length(cost2_grid),
                      dimnames = list(gamma_grid,
                                      cost2_grid))

### Export labels
write.csv(ratios_grid,"./urgency-ratio-grid.csv")

```

### Bayesian Model for Cost Estimation

#### Model

```{stan cost-model, eval=FALSE, include=TRUE, output.var="my_model"}
data {
  int<lower=1> N;
  int<lower=1> T;
  int<lower=2> nOpt;
  int<lower=1, upper=T> Tsubj[N];
  int<lower=0, upper=nOpt> choice[N, T]; //left or right?
  int<lower=0, upper=80> opt_st[N, T]; //option-state: an easy way to map the choices for choice prob calculation
  int value_lookup[80];
  int state_lookup[52, nOpt];
  matrix<lower=0, upper = 1>[80, 52] prob_weight;
  int<lower=0, upper=80> counterpart[80];
  // real outcome[N, T];  // no lower and upper bounds
}
transformed data {
  vector[nOpt] initV;  // initial values for EV
  initV = rep_vector(0.0, nOpt);
}
parameters {
// Declare all parameters as vectors for vectorizing
  // Hyper(group)-parameters
  vector[2] mu_pr;
  vector<lower=0>[2] sigma;

  // Subject-level parameters (for transformation from hyper to subj parameter)
  vector[N] costL;  // cost_low
  vector[N] costH;    // cost_high
}
model {
  // Hyperparameters
  mu_pr  ~ normal(0, 5); //weakly informative priors
  sigma ~ gamma(2,0.1); //weakly informative priors

  // individual parameters
  for (i in 1:N) {
    //tau[i]   = Phi_approx(mu_pr[1]  + sigma[1]  * tau_pr[i]); //approx Normal CDF + noise
    costL[i] ~ normal(mu_pr[1], sigma[1]);
    costH[i] ~ normal(mu_pr[2], sigma[2]);
  }

  // subject loop and trial loop
  for (i in 1:N) {
    vector[nOpt] ev; // expected value
    vector[4] value; // vector of value option, lookup table
    matrix[80, Tsubj[i]] ch; 
    matrix[52, Tsubj[i]] st;
    int round_back; // backwards counter for induction
    real weighted_value;
    
    ev = initV;

    // Declaring values for each option, lookup table (make it loop later)
    value[1] = 2 - costH[i];
    value[2] = 1 - costH[i];
    value[3] = 1 - costL[i];
    value[4] = 2 - costL[i];


    // Backwards induction
    //  fill in column in "ch" with the values
    for(option in 1:80) {
      // first column is choice: find in tasks table the value of that choice
      // lookup is a vector that tells you which cost correspondends to each choice
      ch[option, Tsubj[i]] = value[value_lookup[option]];
    }
    //    Create state values for each round
    // state_lookup tells you which choice|state maps onto which state
    // for each state in "S" choose the highest value in "ch" and create another column with the values
    for(state in 1:52) {
      if (ch[state_lookup[state,1], Tsubj[i]] >= ch[state_lookup[state,2], Tsubj[i]]) {
        st[state, Tsubj[i]] = ch[state_lookup[state,1], Tsubj[i]];
      } else if (ch[state_lookup[state,1], Tsubj[i]] < ch[state_lookup[state,2], Tsubj[i]]) {
        st[state, Tsubj[i]] = ch[state_lookup[state,2], Tsubj[i]];
      }
    }
        // compute action probabilities
        ev[1] = ch[opt_st[i, Tsubj[i]], Tsubj[i]];
        ev[2] = ch[counterpart[opt_st[i, Tsubj[i]]], Tsubj[i]];
        choice[i, Tsubj[i]] ~ categorical_logit(ev);
        
        for (t in 1:(Tsubj[i]-1)) {
          round_back = Tsubj[i] - t;
          for(option in 1:80) {
            // use action probabilities
            weighted_value = dot_product(prob_weight[option], col(st, (round_back + 1)) );
            ch[option, round_back] = value[value_lookup[option]] + weighted_value;
          }
          for(state in 1:52) {
            if (ch[state_lookup[state,1], round_back] >= ch[state_lookup[state,2], round_back]) {
              st[state, round_back] = ch[state_lookup[state,1], round_back];
            } else if (ch[state_lookup[state,1], round_back] < ch[state_lookup[state,2], round_back]) {
              st[state, round_back] = ch[state_lookup[state,2], round_back];
            }
          }
          // compute action probabilities
          ev[1] = ch[opt_st[i, round_back], round_back];
          ev[2] = ch[counterpart[opt_st[i, round_back]], round_back];
          choice[i, round_back] ~ categorical_logit(ev);
        }
  }
  
}
```

#### Model Data

```{r cost-model-data, eval=FALSE, include=TRUE}
# Data Wrangling ----------------------------------------------------------
load("Prolific-pilot1.Rdata")
data <- pilot1[!is.na(pilot1$participant),]
data <- data %>%
  group_by(participant) %>%
  mutate(COUNTER = row_number())
Tsubj <- data %>%
  group_by(participant) %>%
  summarize(n = n())

# Creating Choice set ----------------------------------------------------------
choices <- c("HHU", "HH0", "HLU", "HL0", "LHU", "LH0", "LLU", "LL0")
ch <- expand.grid(choices, choices)
# Fix factor levels for next step
levels(ch$Var2) = append(choices, "TR")
for (chr in choices) {
  ch <- rbind(ch,c(chr,NA))
  ch <- rbind(ch,c(chr,"TR"))
}

# Creating State set ----------------------------------------------------------
S <- as.data.frame(t(combn(choices,2)))
for (chr in choices) {
  # Add states with two of the same task
  S <- rbind(S,c(chr,chr))
  # Add states with only one task transition to one task
  S <- rbind(S,c(chr,NA))
  # Add states with only one task transitioning to two tasks
  S <- rbind(S,c(chr,"TR"))
}

# Create Transition Set ----------------------------------------------------------
# Transition Probabilities
choices <- paste(ch$Var1,ch$Var2,sep = "x")
states <- paste(S$V1,S$V2,sep = "x")
transitions <- expand.grid(choices,states)
# Remaining task (last 3 characters) is in the next state, but no NA
transitions$prob <- NA
## Non-urgent tasks:
## Note this is also transitioning to NA and TR states. Fix it in line 91 and __
transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$prob <- 
  str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var2,
             pattern = str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-3), pattern = "0"),]$Var1,-3))
## Urgent tasks: 
### If HXU -> LXU
transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
              str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$prob <-
  str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                 str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var2,
             pattern = paste("L",
                             str_sub(transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                                   str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "H"),]$Var1,
                                     start = -2,
                                     end = -2),"U",sep = ""))
### If LXU -> NA
## first change all transitions to false
transitions[str_detect(str = str_sub(transitions$Var2,-3), pattern = "NA"),]$prob <- FALSE
# Now calculate other transitions
transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
              str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$prob <-
  str_detect(str = transitions[str_detect(str = str_sub(transitions$Var1,-1), pattern = "U") & 
                                 str_detect(str = str_sub(transitions$Var1,start = -3, end = -3), pattern = "L"),]$Var2,
             pattern = "NA")

## Single task with transition to single task:
transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$prob <-
  str_detect(str = transitions[str_detect(str = transitions$Var1, pattern = "NA"),]$Var2,
             pattern = "TR")
## Single task with transition to single task:
transitions[str_detect(str = transitions$Var1, pattern = "TR"),]$prob <- 0

#Transforming them into probabilities by dividing by 8
transitions$prob <- as.numeric(transitions$prob)
transitions$prob <- transitions$prob / 8

## Single task with transition to single task:
transitions[str_detect(str = transitions$Var1, pattern = "TR") &
              str_length(transitions$Var2) == 7,]$prob <- 1/36
# We need to change "transitions" for simplicity of search
transitions$choice <- substr(transitions$Var1, 1, 3)
transitions$non_choice <- substr(transitions$Var1, 5, 7)
transitions[transitions$non_choice == "NA",]$non_choice <- NA
transitions$state2_1 <- substr(transitions$Var2, 1, 3)
transitions$state2_2 <- substr(transitions$Var2, 5, 7)
transitions[transitions$state2_2 == "NA",]$state2_2 <- NA


# create choice[N,T] ----------------------------------------------------------
data$choice_bin <- 1
data[data$choice == "R",]$choice_bin <- 2
choice <- data %>%
  select(participant, COUNTER, choice_bin) %>%
  group_by(COUNTER) %>%
  spread(COUNTER, choice_bin)
# Take NAs out
choice[is.na(choice)] <- 0


# Option-State ------------------------------------------------------------
## an easy way to map the choices for choice prob calculation
ch_opt_translation <- ch[,c(1:2)]
ch_opt_translation$index <- c(1:length(ch_opt_translation$Var1))
ch_opt_translation$i1 <- NA
ch_opt_translation$e1 <- NA
ch_opt_translation$i2 <- NA
ch_opt_translation$e2 <- NA

ch_opt_translation$u1 <- grepl("U", ch_opt_translation$Var1)
ch_opt_translation$u2 <- grepl("U", ch_opt_translation$Var2)
ch_opt_translation[is.na(ch_opt_translation$Var2), ]$u2 <- NA
ch_opt_translation$u1 <- as.numeric(ch_opt_translation$u1) + 1
ch_opt_translation$u2 <- as.numeric(ch_opt_translation$u2) + 1

ch_opt_translation$i1 <- ifelse(substring(ch_opt_translation$Var1, 1, 1) == "H", 2, 1)
ch_opt_translation$i2 <- ifelse(substring(ch_opt_translation$Var2, 1, 1) == "H", 2, 1)
ch_opt_translation[is.na(ch_opt_translation$Var2), ]$i2 <- NA

ch_opt_translation$e1 <- ifelse(substring(ch_opt_translation$Var1, 2, 2) == "H", 2, 1)
ch_opt_translation$e2 <- ifelse(substring(ch_opt_translation$Var2, 2, 2) == "H", 2, 1)
ch_opt_translation[is.na(ch_opt_translation$Var2), ]$e2 <- NA

ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$i2 <- 
  ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$i1
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$e2 <- 
  ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$e1
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$u2 <- 
  ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$u1

ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$i1 <- NA
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$e1 <- NA
ch_opt_translation[grepl("TR", ch_opt_translation$Var2), ]$u1 <- NA

data <- merge(x = data,
              y = ch_opt_translation[,-c(1:2)],
              by = c("u1", "u2", "e1", "e2", "i1", "i2"))

opt_st <- data %>%
  select(participant, COUNTER, index) %>%
  group_by(COUNTER) %>%
  spread(COUNTER, index)
# Take NAs out
opt_st[is.na(opt_st)] <- 0

ch_opt_translation$value <- 0
ch_opt_translation[grepl("HH", ch_opt_translation$Var1),]$value = 1
ch_opt_translation[grepl("LH", ch_opt_translation$Var1),]$value = 2
ch_opt_translation[grepl("LL", ch_opt_translation$Var1),]$value = 3
ch_opt_translation[grepl("HL", ch_opt_translation$Var1),]$value = 4


S$st_index <- c(1:nrow(S))
S <- merge(x = S,
           y = ch_opt_translation[,c(1:3)],
           by.x = c("V1", "V2"),
           by.y = c("Var1", "Var2"))
ch_opt_translation$index2 <- ch_opt_translation$index
S <- merge(x = S, 
           y = ch_opt_translation[,c(1:2,11)],
           by.x = c("V2", "V1"),
           by.y = c("Var1", "Var2"),
           all.x = TRUE)
state_lookup <- S %>%
  select(st_index, index, index2)
# Take NAs out
state_lookup[is.na(state_lookup$index2),]$index2 <- state_lookup[is.na(state_lookup$index2),]$index
# Order state_lookup
state_lookup <- state_lookup %>%
  arrange(order_by = st_index)

prob_weight <- merge(x = transitions,
                      y = ch_opt_translation[,c(1:3)],
                      by.x = c("choice", "non_choice"),
                      by.y = c("Var1", "Var2"))
prob_weight <- merge(x = prob_weight,
                     y = ch_opt_translation[,c(1:2,11)],
                     by.x = c("state2_1", "state2_2"),
                     by.y = c("Var1", "Var2"))


# Map Choices to States ---------------------------------------------------
choice_state <- as.data.frame(c(1:80))
choice_state <- merge(x = choice_state,
                      y = state_lookup[,-c(3)],
                      by.x = c("c(1:80)"),
                      by.y = c("index"),
                      all.x = TRUE)
choice_state <- merge(x = choice_state,
                      y = state_lookup[,-c(2)],
                      by.x = c("c(1:80)"),
                      by.y = c("index2"),
                      all.x = TRUE)
choice_state$st_index <- choice_state$st_index.x
choice_state[is.na(choice_state$st_index.x),]$st_index <- choice_state[is.na(choice_state$st_index.x),]$st_index.y
choice_state <- choice_state %>%
  select(`c(1:80)`, st_index)
prob_weight <- merge(x=prob_weight,
                     y=choice_state,
                     by.x = "index2",
                     by.y= "c(1:80)")
prob_weight <- prob_weight %>%
  select(index, st_index, prob)

prob_weight <- prob_weight %>%
  group_by(index) %>%
  spread(st_index, prob)


# Counterpart choice|states -----------------------------------------------
counterpart <- merge(x = choice_state, y = state_lookup[,-c(1)], by.x = "c(1:80)", by.y = "index", all.x = TRUE)
counterpart <- merge(x = counterpart, y = state_lookup[,-c(1)], by.x = "c(1:80)", by.y = "index2", all.x = TRUE)
counterpart$index_ALL <- counterpart$index
counterpart[is.na(counterpart$index),]$index_ALL <- counterpart[is.na(counterpart$index),]$index2
counterpart <- counterpart %>%
  select(index_ALL)

model_data <- list( N = length(unique(data$participant)), #number of part
                    T = max(data$COUNTER), # number of max rounds
                    nOpt = 2,
                    Tsubj = Tsubj$n, # number of round
                    choice = choice[,c(-1)],
                    opt_st = opt_st[,c(-1)],
                    value_lookup = ch_opt_translation$value,
                    state_lookup = state_lookup[,c(-1)],
                    prob_weight = prob_weight[,c(-1)],
                    counterpart = counterpart$index_ALL) # transition probabilities

```

#### Estimation

```{r cost-model-estimation, eval=FALSE, include=TRUE}

load("prolific1StanData.Rdata")
# my_model <- stan_model(file = "recipes/hier-bayes-simple.stan", verbose = TRUE)
Prolific1_Stan_results <- sampling(object = my_model, data = model_data,
                                   iter = 1000,
                                   chains = 1,
                                   cores = 3)

```

## Questionnaires {#questionnaires}

The remaining pages include the questionnaires we will administer to participants in the next experiment iterations.
